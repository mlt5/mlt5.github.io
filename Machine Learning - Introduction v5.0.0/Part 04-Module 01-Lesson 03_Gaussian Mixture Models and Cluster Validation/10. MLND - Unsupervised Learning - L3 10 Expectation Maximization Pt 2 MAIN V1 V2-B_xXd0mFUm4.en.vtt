WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.769
The third step of the expectation maximization algorithm for

00:00:04.769 --> 00:00:10.251
GMM clustering is to take whatever what was produced in step two,

00:00:10.252 --> 00:00:15.319
which is the memberships of all the points to all the clusters,

00:00:15.319 --> 00:00:16.920
the two clusters that we have,

00:00:16.920 --> 00:00:22.480
and use those to come up with new parameters for the Gaussians.

00:00:22.480 --> 00:00:25.530
And so, the entire purpose of step three is

00:00:25.530 --> 00:00:28.560
to use whatever is step two produced as input,

00:00:28.559 --> 00:00:31.469
and the goal is to fill up this small table,

00:00:31.469 --> 00:00:33.179
which is to say for Cluster A,

00:00:33.179 --> 00:00:36.750
we need the new mean and a new variance,

00:00:36.750 --> 00:00:38.715
and we do the same with Cluster B.

00:00:38.715 --> 00:00:43.935
So, all that it is is just coming up with these four values here.

00:00:43.935 --> 00:00:46.770
So, let's go and do that right now.

00:00:46.770 --> 00:00:49.710
And so, the new mean for Cluster A,

00:00:49.710 --> 00:00:52.259
given the result of step two,

00:00:52.259 --> 00:00:56.804
comes from calculating the weighted average of all of these points.

00:00:56.804 --> 00:01:02.144
The weighted average does not only account for the parameters of each point,

00:01:02.145 --> 00:01:05.825
but also account for how much it belongs.

00:01:05.825 --> 00:01:07.745
So it's membership to Cluster A,

00:01:07.745 --> 00:01:09.745
and that's these terms here.

00:01:09.745 --> 00:01:11.189
And so let's calculate that.

00:01:11.189 --> 00:01:13.784
So, this is the table resulting from step two,

00:01:13.784 --> 00:01:18.959
where we filled up all the memberships for all the points in both clusters.

00:01:18.959 --> 00:01:21.839
And the numbers are slightly different here because this is

00:01:21.840 --> 00:01:27.155
the correct calculation where we accounted for the mixing coefficient.

00:01:27.155 --> 00:01:29.730
This value here is just arbitrary.

00:01:29.730 --> 00:01:34.350
I added it just to showcase that it's not always 99 percent accuracy.

00:01:34.349 --> 00:01:39.144
It can be 55 and 40 and 30 and anything between zero and one.

00:01:39.144 --> 00:01:43.439
And so, this is how we calculate the weighted average.

00:01:43.439 --> 00:01:47.564
And it's basically going to each row here and multiplying

00:01:47.564 --> 00:01:53.549
this cell by an array or a vector containing these two values,

00:01:53.549 --> 00:01:55.064
and that would be this term.

00:01:55.064 --> 00:01:57.138
And then we do that for each row,

00:01:57.138 --> 00:02:00.314
summing them up and then that would be this entire term.

00:02:00.314 --> 00:02:03.599
And then, we sum up the values of this column,

00:02:03.599 --> 00:02:06.044
and that would be what we divide by.

00:02:06.045 --> 00:02:08.340
And then we calculate that,

00:02:08.340 --> 00:02:11.295
and we get the you mean for Cluster A.

00:02:11.294 --> 00:02:15.030
We do the same for a Cluster B, except here,

00:02:15.030 --> 00:02:18.405
we'll be looking at the weighted average of these points here,

00:02:18.405 --> 00:02:21.175
and the calculation is almost the same.

00:02:21.175 --> 00:02:22.875
And so, we have the new values.

00:02:22.875 --> 00:02:24.694
To calculate the new variance,

00:02:24.694 --> 00:02:26.310
we do something similar.

00:02:26.310 --> 00:02:28.229
So this term here,

00:02:28.229 --> 00:02:35.429
this looks eerily similar to the regular calculation of how variance is calculated,

00:02:35.430 --> 00:02:37.830
except here, we're doing a weighted version of it.

00:02:37.830 --> 00:02:44.070
So we're multiplying here by the membership to each cluster.

00:02:44.069 --> 00:02:48.344
And so, if we plug in the data that we got from step two,

00:02:48.344 --> 00:02:53.655
we can get a new value for the new variance for Cluster A,

00:02:53.655 --> 00:02:57.150
and we can do the same with Cluster B as well.

00:02:57.150 --> 00:02:59.409
And so now step three is complete.

00:02:59.409 --> 00:03:02.574
We have the two new Gaussians,

00:03:02.574 --> 00:03:05.375
and we can compare them with the old Gaussians.

00:03:05.375 --> 00:03:07.080
Let's look at this graph here.

00:03:07.080 --> 00:03:08.450
So these are the old Gaussians.

00:03:08.449 --> 00:03:12.491
And so, this is an overlay of the old and the new Gaussians.

00:03:12.491 --> 00:03:15.149
And we can see that the green Gaussian started

00:03:15.150 --> 00:03:18.105
moving to the top and right here a little bit.

00:03:18.104 --> 00:03:20.209
So this is one step.

00:03:20.210 --> 00:03:21.300
Now, if it doesn't converge,

00:03:21.300 --> 00:03:23.550
we'll continue to do five, 10, 20,

00:03:23.550 --> 00:03:27.510
30 steps like this until we converge into

00:03:27.509 --> 00:03:32.909
a Gaussian mixture that produces a good result for the problem.

00:03:32.909 --> 00:03:37.210
The fourth step is to evaluate the log-likelihood.

00:03:37.210 --> 00:03:42.890
And the log-likelihood sums for all clusters here.

00:03:42.889 --> 00:03:46.657
This is the mixing coefficient related to this cluster,

00:03:46.657 --> 00:03:49.934
and then this is the normal distribution of each point,

00:03:49.935 --> 00:03:53.039
given the parameter of the cluster,

00:03:53.039 --> 00:03:57.689
and this term as a whole is called log-likelihood.

00:03:57.689 --> 00:04:02.264
And basically, what this says is that the higher this number is,

00:04:02.264 --> 00:04:06.884
the more sure we are that the mixer model that we've generated

00:04:06.884 --> 00:04:11.844
is responsible for creating the data or fits the dataset that we have.

00:04:11.844 --> 00:04:16.214
And so, the purpose here is to maximize this value, this term.

00:04:16.214 --> 00:04:21.301
We do that by choosing the parameters of the Gaussians,

00:04:21.302 --> 00:04:24.135
including the mixing coefficient,

00:04:24.134 --> 00:04:27.824
the mean, and the variance of each Gaussian.

00:04:27.824 --> 00:04:30.104
The better parameters we pick,

00:04:30.105 --> 00:04:32.830
the higher the value of this entire term will be.

00:04:32.829 --> 00:04:35.954
And then we do that until the algorithm converges,

00:04:35.954 --> 00:04:42.714
until it reaches a maximum or it starts to increase by a tiny fraction each step.

00:04:42.714 --> 00:04:45.558
And so, that's where we can stop the algorithm and say,

00:04:45.559 --> 00:04:47.750
"Alright. We have converged.

00:04:47.750 --> 00:04:51.269
Let's choose those Gaussians as

00:04:51.269 --> 00:04:55.745
the components of a Gaussian mixture model and that's cluster based on them."

00:04:55.745 --> 00:04:57.540
So, this is the final step of

00:04:57.540 --> 00:05:02.615
the expectation maximization algorithm to do GMM clustering.

00:05:02.615 --> 00:05:05.060
In the next video, we'll look at a visual example,

00:05:05.060 --> 00:05:08.939
where we'll visualize step number one and two and three,

00:05:08.939 --> 00:05:12.000
the entire process until convergence.


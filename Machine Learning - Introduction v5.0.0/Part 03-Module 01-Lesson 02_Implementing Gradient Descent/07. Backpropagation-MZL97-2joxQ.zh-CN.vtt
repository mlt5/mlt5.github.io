WEBVTT
Kind: captions
Language: zh-CN

00:00:00.590 --> 00:00:03.080
对于这个多层神经网络

00:00:03.080 --> 00:00:05.950
我们仍希望使用梯度下降法对其训练

00:00:05.950 --> 00:00:09.330
此前我们已经学过 如何计算输出节点的误差项

00:00:09.330 --> 00:00:11.919
借助梯度下降法 我们可以使用此误差项来训练

00:00:11.919 --> 00:00:13.539
隐藏层到输出层的权重

00:00:13.539 --> 00:00:15.899
但要训练输入层到隐藏层的权重

00:00:15.900 --> 00:00:19.690
我们需要知道隐藏层单元对应的误差项

00:00:19.690 --> 00:00:22.800
那么如何求取梯度下降步骤所需的误差项呢？

00:00:22.800 --> 00:00:26.539
此前 我们通过对误差平方求关于 “输入层到输出层权重” 的偏导

00:00:26.539 --> 00:00:29.769
来计算误差项

00:00:29.769 --> 00:00:33.850
若加入隐藏层 我们使用链式法则时会发现

00:00:33.850 --> 00:00:37.050
隐藏层误差项与输出层误差项成正比

00:00:37.049 --> 00:00:38.820
比例系数由两层之间的权重决定

00:00:38.820 --> 00:00:40.460
这是有道理的

00:00:40.460 --> 00:00:42.789
隐藏层单元与输出节点连接越强

00:00:42.789 --> 00:00:46.089
则对最终输出值误差项的影响越大

00:00:46.090 --> 00:00:48.430
图中可以看到误差项乘以权重值

00:00:48.429 --> 00:00:51.469
这与神经网络输入值的正向传播类似

00:00:51.469 --> 00:00:54.299
即输入值乘以层间权重值

00:00:54.299 --> 00:00:56.599
相对于输入值的正向传播

00:00:56.600 --> 00:01:00.070
这是误差项的反向传播

00:01:00.070 --> 00:01:03.929
其实可以将此看作 首先反转神经网络

00:01:03.929 --> 00:01:06.340
然后将误差项用作输入值

00:01:06.340 --> 00:01:08.329
此方法称为反向传播

00:01:08.329 --> 00:01:11.450
即使加深层数 这个过程也是一样的

00:01:11.450 --> 00:01:14.299
只需逐层不断传播误差项

00:01:14.299 --> 00:01:17.810
反向传播是训练神经网络的基础原理

00:01:17.810 --> 00:01:19.629
因此对于构建深度学习模型

00:01:19.629 --> 00:01:21.439
理解反向传播至关重要


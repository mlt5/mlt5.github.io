WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:02.033
Vimos antes
com o perceptron XOR

00:00:02.067 --> 00:00:04.734
que colocar uma 2ª camada
permite que o modelo

00:00:04.767 --> 00:00:08.033
ache soluções para problemas
linearmente inseparáveis.

00:00:08.067 --> 00:00:11.100
Aqui está um exemplo
de um perceptron multicamadas,

00:00:11.133 --> 00:00:14.334
com 3 unidades de entrada,
uma de saída,

00:00:14.367 --> 00:00:16.300
e duas unidades no meio.

00:00:16.334 --> 00:00:18.868
A camada do meio
chama-se camada oculta.

00:00:18.901 --> 00:00:21.734
Calculando a saída desta rede
é como antes,

00:00:21.767 --> 00:00:24.968
só que agora, as ativações
da camada oculta são usadas

00:00:25.000 --> 00:00:27.200
como a entrada
para a camada de saída.

00:00:27.234 --> 00:00:30.200
A entrada para a camada oculta
é a mesma de antes:

00:00:30.234 --> 00:00:34.367
Estes pesos vezes os valores de
entrada mais um termo de tendência.

00:00:34.400 --> 00:00:37.968
E, como antes, de novo,
você usa uma função de ativação,

00:00:38.000 --> 00:00:41.267
como um sigmóide, para calcular
a saída da camada oculta.

00:00:41.300 --> 00:00:44.701
As ativações da camada oculta
passam para a camada de saída

00:00:44.734 --> 00:00:46.133
com o 2º conjunto de pesos,

00:00:46.167 --> 00:00:49.767
e usam uma função de ativação
para obter a saída da rede.

00:00:49.801 --> 00:00:51.501
Empilhar mais camadas assim

00:00:51.534 --> 00:00:54.367
ajuda a rede de aprender
padrões mais complexos.

00:00:54.400 --> 00:00:56.634
Daí vem o nome
da aprendizagem profunda,

00:00:56.667 --> 00:00:58.234
e o que a faz tão forte.

00:00:58.267 --> 00:01:00.367
Pilhas profundas
de camadas ocultas.


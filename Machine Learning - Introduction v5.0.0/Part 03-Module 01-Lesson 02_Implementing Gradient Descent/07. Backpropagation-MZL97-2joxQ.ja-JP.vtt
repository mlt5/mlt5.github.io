WEBVTT
Kind: captions
Language: ja-JP

00:00:00.590 --> 00:00:03.080
今度は複数の層を
扱います

00:00:03.080 --> 00:00:05.950
やはりネットワークを
勾配降下法でトレーニングします

00:00:05.950 --> 00:00:09.330
以前から 出力ユニットの誤差を
計算する方法はわかっています

00:00:09.330 --> 00:00:11.919
この誤差を勾配降下法で
使用して

00:00:11.919 --> 00:00:13.539
隠れ層を出力の重みにトレーニングできます

00:00:13.539 --> 00:00:15.899
しかし 入力を
隠れ重みにトレーニングするには

00:00:15.900 --> 00:00:19.690
隠れ層のユニットによって生じた
誤差を知る必要があります

00:00:19.690 --> 00:00:22.800
これらの誤差を見つけて
勾配降下法ステップで使用するには どうしたらよいでしょうか

00:00:22.800 --> 00:00:26.539
以前 入力層と出力層の間の重みを考慮して
二乗誤差の微分係数を取ることによって

00:00:26.539 --> 00:00:29.769
誤差を
発見しました

00:00:29.769 --> 00:00:33.850
これを隠れ層で
連鎖法則を使用して行う場合

00:00:33.850 --> 00:00:37.050
ユニットの誤差は出力層の誤差に
ユニット間の重みを

00:00:37.049 --> 00:00:38.820
掛けた者に比例することがわかります

00:00:38.820 --> 00:00:40.460
これは筋が通っています

00:00:40.460 --> 00:00:42.789
出力ノードとの
結び付きが強いユニットは

00:00:42.789 --> 00:00:46.089
最終的な出力の誤差に
より多く貢献します

00:00:46.090 --> 00:00:48.430
そこで誤差に
重みを掛けます

00:00:48.429 --> 00:00:51.469
これは入力をネットワーク経由で
伝播するのと同じ方法であり

00:00:51.469 --> 00:00:54.299
入力に層の間の
重みを掛けます

00:00:54.299 --> 00:00:56.599
入力を前方に
伝播する代わりに

00:00:56.600 --> 00:01:00.070
誤差をネットワーク経由で
後方に伝播します

00:01:00.070 --> 00:01:03.929
このプロセスは
ネットワークを逆転して

00:01:03.929 --> 00:01:06.340
誤差を入力として使用します

00:01:06.340 --> 00:01:08.329
この方法は逆伝播と呼ばれます

00:01:08.329 --> 00:01:11.450
このプロセスは
層を追加しても同じように機能します

00:01:11.450 --> 00:01:14.299
層を通じて誤差を
伝播し続けるだけです

00:01:14.299 --> 00:01:17.810
逆伝搬はニューラルネットワークの
学習の基盤です

00:01:17.810 --> 00:01:19.629
ディープラーニングモデルを
構築する場合

00:01:19.629 --> 00:01:21.439
これを理解することは
非常に重要です


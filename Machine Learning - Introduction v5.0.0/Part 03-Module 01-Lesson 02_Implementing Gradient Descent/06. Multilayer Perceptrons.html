<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Multilayer Perceptrons
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Implementing Gradient Descent
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Mean Squared Error Function.html">
       01. Mean Squared Error Function
      </a>
     </li>
     <li class="">
      <a href="02. Gradient Descent.html">
       02. Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="03. Gradient Descent The Math.html">
       03. Gradient Descent: The Math
      </a>
     </li>
     <li class="">
      <a href="04. Gradient Descent The Code.html">
       04. Gradient Descent: The Code
      </a>
     </li>
     <li class="">
      <a href="05. Implementing Gradient Descent.html">
       05. Implementing Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="06. Multilayer Perceptrons.html">
       06. Multilayer Perceptrons
      </a>
     </li>
     <li class="">
      <a href="07. Backpropagation.html">
       07. Backpropagation
      </a>
     </li>
     <li class="">
      <a href="08. Implementing Backpropagation.html">
       08. Implementing Backpropagation
      </a>
     </li>
     <li class="">
      <a href="09. Further Reading.html">
       09. Further Reading
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          06. Multilayer Perceptrons
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
         <p>
          Multilayer perceptrons
         </p>
        </h3>
        <video controls="">
         <source src="06. Multilayer perceptrons-Rs9petvTBLk.mp4" type="video/mp4"/>
         <track default="false" kind="subtitles" label="US" src="06. Multilayer perceptrons-Rs9petvTBLk.en-US.vtt" srclang="US"/>
         <track default="false" kind="subtitles" label="JP" src="06. Multilayer perceptrons-Rs9petvTBLk.ja-JP.vtt" srclang="JP"/>
         <track default="false" kind="subtitles" label="BR" src="06. Multilayer perceptrons-Rs9petvTBLk.pt-BR.vtt" srclang="BR"/>
         <track default="false" kind="subtitles" label="CN" src="06. Multilayer perceptrons-Rs9petvTBLk.zh-CN.vtt" srclang="CN"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="implementing-the-hidden-layer">
          Implementing the hidden layer
         </h1>
         <h5 id="prerequisites">
          Prerequisites
         </h5>
         <p>
          Below, we are going to walk through the math of neural networks in a multilayer perceptron. With multiple perceptrons, we are going to move to using vectors and matrices. To brush up, be sure to view the following:
         </p>
         <ol>
          <li>
           Khan Academy's
           <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra" rel="noopener noreferrer" target="_blank">
            introduction to vectors
           </a>
           .
          </li>
          <li>
           Khan Academy's
           <a href="https://www.khanacademy.org/math/precalculus/precalc-matrices" rel="noopener noreferrer" target="_blank">
            introduction to matrices
           </a>
           .
          </li>
         </ol>
         <h5 id="derivation">
          Derivation
         </h5>
         <p>
          Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices:
          <span class="mathquill ud-math">
           w_{ij}
          </span>
          where
          <span class="mathquill ud-math">
           i
          </span>
          denotes input units and
          <span class="mathquill ud-math">
           j
          </span>
          are the hidden units.
         </p>
         <p>
          For example, the following image shows our network, with its input units labeled
          <span class="mathquill ud-math">
           x_1, x_2,
          </span>
          and
          <span class="mathquill ud-math">
           x_3
          </span>
          , and its hidden nodes labeled
          <span class="mathquill ud-math">
           h_1
          </span>
          and
          <span class="mathquill ud-math">
           h_2
          </span>
          :
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/network-with-labeled-nodes.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The lines indicating the weights leading to
          <span class="mathquill ud-math">
           h_1
          </span>
          have been colored differently from those leading to
          <span class="mathquill ud-math">
           h_2
          </span>
          just to make it easier to read.
         </p>
         <p>
          Now to index the weights, we take the input unit number for the
          <span class="mathquill ud-math">
           _i
          </span>
          and the hidden unit number for the
          <span class="mathquill ud-math">
           _j.
          </span>
          That gives us
         </p>
         <p>
          <span class="mathquill ud-math">
           w_{11}
          </span>
         </p>
         <p>
          for the weight leading from
          <span class="mathquill ud-math">
           x_1
          </span>
          to
          <span class="mathquill ud-math">
           h_1
          </span>
          , and
         </p>
         <p>
          <span class="mathquill ud-math">
           w_{12}
          </span>
         </p>
         <p>
          for the weight leading from
          <span class="mathquill ud-math">
           x_1
          </span>
          to
          <span class="mathquill ud-math">
           h_2
          </span>
          .
         </p>
         <p>
          The following image includes all of the weights between the input layer and the hidden layer, labeled with their appropriate
          <span class="mathquill ud-math">
           w_{ij}
          </span>
          indices:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/network-with-labeled-weights.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Before, we were able to write the weights as an array, indexed as
          <span class="mathquill ud-math">
           w_i
          </span>
          .
         </p>
         <p>
          But now, the weights need to be stored in a
          <strong>
           matrix
          </strong>
          , indexed as
          <span class="mathquill ud-math">
           w_{ij}
          </span>
          . Each
          <strong>
           row
          </strong>
          in the matrix will correspond to the weights
          <strong>
           leading out
          </strong>
          of a
          <strong>
           single input unit
          </strong>
          , and each
          <strong>
           column
          </strong>
          will correspond to the weights
          <strong>
           leading in
          </strong>
          to a
          <strong>
           single hidden unit
          </strong>
          . For our three input units and two hidden units, the weights matrix looks like this:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Weights matrix for 3 input units and 2 hidden units" class="img img-fluid" src="img/multilayer-diagram-weights.png"/>
          <figcaption class="figure-caption">
           <p>
            Weights matrix for 3 input units and 2 hidden units
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Be sure to compare the matrix above with the diagram shown before it so you can see where the different weights in the network end up in the matrix.
         </p>
         <p>
          To initialize these weights in NumPy, we have to provide the shape of the matrix. If
          <code>
           features
          </code>
          is a 2D array containing the input data:
         </p>
         <pre><code class="python language-python"># Number of records and input units
n_records, n_inputs = features.shape
# Number of hidden units
n_hidden = 2
weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))</code></pre>
         <p>
          This creates a 2D array (i.e. a matrix) named
          <code>
           weights_input_to_hidden
          </code>
          with dimensions
          <code>
           n_inputs
          </code>
          by
          <code>
           n_hidden
          </code>
          . Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit,
          <span class="mathquill ud-math">
           h_j
          </span>
          , we need to calculate the following:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/hidden-layer-weights.gif"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          To do that, we now need to use
          <a href="https://en.wikipedia.org/wiki/Matrix_multiplication" rel="noopener noreferrer" target="_blank">
           matrix multiplication
          </a>
          . If your linear algebra is rusty, I suggest taking a look at the suggested resources in the prerequisites section. For this part though, you'll only need to know how to multiply a matrix with a vector.
         </p>
         <p>
          In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit,
          <span class="mathquill ud-math">
           j = 1
          </span>
          , you'd take the dot product of the inputs with the first column of the weights matrix, like so:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Calculating the input to the first hidden unit with the first column of the weights matrix. " class="img img-fluid" src="img/input-times-weights.png"/>
          <figcaption class="figure-caption">
           <p>
            Calculating the input to the first hidden unit with the first column of the weights matrix.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/codecogseqn-2.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.
         </p>
         <p>
          In NumPy, you can do this for all the inputs and all the outputs at once using
          <code>
           np.dot
          </code>
         </p>
         <pre><code class="python language-python">hidden_inputs = np.dot(inputs, weights_input_to_hidden)</code></pre>
         <p>
          You could also define your weights matrix such that it has dimensions
          <code>
           n_hidden
          </code>
          by
          <code>
           n_inputs
          </code>
          then multiply like so where the inputs form a
          <em>
           column vector
          </em>
          :
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/inputs-matrix.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          <strong>
           Note:
          </strong>
          The weight indices have changed in the above image and no longer match up with the labels used in the earlier diagrams. That's because, in matrix notation, the row index always precedes the column index, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row. If we
          <em>
           were
          </em>
          to use the labels from the earlier diagram, the weights would fit into the matrix in the following locations:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Weight matrix shown with labels matching earlier diagrams." class="img img-fluid" src="img/weight-label-reference.gif"/>
          <figcaption class="figure-caption">
           <p>
            Weight matrix shown with labels matching earlier diagrams.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Remember, the above is
          <strong>
           not
          </strong>
          a correct view of the
          <strong>
           indices
          </strong>
          , but it uses the labels from the earlier neural net diagrams to show you where each weight ends up in the matrix.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The important thing with matrix multiplication is that
          <em>
           the dimensions match
          </em>
          . For matrix multiplication to work, there has to be the same number of elements in the dot products. In the first example, there are three columns in the input vector, and three rows in the weights matrix. In the second example, there are three columns in the weights matrix and three rows in the input vector. If the dimensions don't match, you'll get this:
         </p>
         <pre><code class="python language-python"># Same weights and features as above, but swapped the order
hidden_inputs = np.dot(weights_input_to_hidden, features)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-11-1bfa0f615c45&gt; in &lt;module&gt;()
----&gt; 1 hidden_in = np.dot(weights_input_to_hidden, X)

ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)</code></pre>
         <p>
          The dot product can't be computed for a 3x2 matrix and 3-element array.  That's because the 2 columns in the matrix don't match the number of elements in the array.  Some of the dimensions that could work would be the following:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/matrix-mult-3.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          The rule is that if you're multiplying an array from the left, the array must have the same number of elements as there are rows in the matrix. And if you're multiplying the
          <em>
           matrix
          </em>
          from the left, the number of columns in the matrix must equal the number of elements in the array on the right.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="making-a-column-vector">
          Making a column vector
         </h3>
         <p>
          You see above that sometimes you'll want a column vector, even though by default NumPy arrays work like row vectors. It's possible to get the transpose of an array like so
          <code>
           arr.T
          </code>
          , but for a 1D array, the transpose will return a row vector. Instead, use
          <code>
           arr[:,None]
          </code>
          to create a column vector:
         </p>
         <pre><code class="python language-python">print(features)
&gt; array([ 0.49671415, -0.1382643 ,  0.64768854])

print(features.T)
&gt; array([ 0.49671415, -0.1382643 ,  0.64768854])

print(features[:, None])
&gt; array([[ 0.49671415],
       [-0.1382643 ],
       [ 0.64768854]])</code></pre>
         <p>
          Alternatively, you can create arrays with two dimensions. Then, you can use
          <code>
           arr.T
          </code>
          to get the column vector.
         </p>
         <pre><code class="python language-python">np.array(features, ndmin=2)
&gt; array([[ 0.49671415, -0.1382643 ,  0.64768854]])

np.array(features, ndmin=2).T
&gt; array([[ 0.49671415],
       [-0.1382643 ],
       [ 0.64768854]])</code></pre>
         <p>
          I personally prefer keeping all vectors as 1D arrays, it just works better in my head.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="programming-quiz">
          Programming quiz
         </h2>
         <p>
          Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.
         </p>
         <p>
          Things to do:
         </p>
         <ul>
          <li>
           Calculate the input to the hidden layer.
          </li>
          <li>
           Calculate the hidden layer output.
          </li>
          <li>
           Calculate the input to the output layer.
          </li>
          <li>
           Calculate the output of the network.
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h4>
          Start Quiz:
         </h4>
         <div>
          <div class="nav nav-tabs nav-fill" id="question-tabs" role="tablist">
           <a aria-controls="256165-multilayer-py" aria-selected="true" class="nav-item nav-link active show" data-toggle="tab" href="#256165-multilayer-py" id="tab-256165-multilayer-py" role="tab">
            multilayer.py
           </a>
           <a aria-controls="256165-solution-py" aria-selected="false" class="nav-item nav-link" data-toggle="tab" href="#256165-solution-py" id="tab-256165-solution-py" role="tab">
            solution.py
           </a>
          </div>
          <div class="tab-content" id="question-tab-contents" style="padding: 20px 0;">
           <div aria-labelledby="tab-256165-multilayer-py" class="tab-pane active show" id="256165-multilayer-py" role="tabpanel">
            <pre><code></code>import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

# Network size
N_input = 4
N_hidden = 3
N_output = 2

np.random.seed(42)
# Make some fake data
X = np.random.randn(4)

weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))


# TODO: Make a forward pass through the network

hidden_layer_in = None
hidden_layer_out = None

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = None
output_layer_out = None

print('Output-layer Output:')
print(output_layer_out)</pre>
           </div>
           <div aria-labelledby="tab-256165-solution-py" class="tab-pane" id="256165-solution-py" role="tabpanel">
            <pre><code></code>import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

# Network size
N_input = 4
N_hidden = 3
N_output = 2

np.random.seed(42)
# Make some fake data
X = np.random.randn(4)

weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))


# TODO: Make a forward pass through the network

hidden_layer_in = np.dot(X, weights_input_to_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)
output_layer_out = sigmoid(output_layer_in)

print('Output-layer Output:')
print(output_layer_out)</pre>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="07. Backpropagation.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('06. Multilayer Perceptrons')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>

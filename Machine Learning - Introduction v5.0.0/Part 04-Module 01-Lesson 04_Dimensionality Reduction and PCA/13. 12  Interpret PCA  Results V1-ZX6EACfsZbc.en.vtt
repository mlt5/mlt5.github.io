WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.754
Now that you've seen how the PCA library works in scikit-learn,

00:00:03.754 --> 00:00:06.574
and you fit it to the handwritten digits dataset,

00:00:06.575 --> 00:00:09.300
we should take a closer look at exactly what

00:00:09.300 --> 00:00:12.554
PCA gives us back and what these different parts mean.

00:00:12.554 --> 00:00:15.394
Let's pick up where we left off in the last video.

00:00:15.394 --> 00:00:18.030
Here, I've read in the necessary libraries and

00:00:18.030 --> 00:00:20.785
split the image data into a label and the features,

00:00:20.785 --> 00:00:22.795
which are the pixels in the actual image.

00:00:22.795 --> 00:00:24.804
Let's take a look at the top 30.

00:00:24.804 --> 00:00:28.054
Now, using the do_PCA function you saw earlier,

00:00:28.054 --> 00:00:29.940
let's create the PCA model,

00:00:29.940 --> 00:00:32.484
as well as the X_PCA component.

00:00:32.484 --> 00:00:34.884
If we look at the PCA model,

00:00:34.884 --> 00:00:39.589
we can hit dot tab to see a whole bunch of things that are available on this.

00:00:39.590 --> 00:00:42.485
Some of the things that I usually pay attention to are,

00:00:42.484 --> 00:00:46.460
so there's explained_variance_ratio, is usually really helpful,

00:00:46.460 --> 00:00:48.674
and the number of components,

00:00:48.674 --> 00:00:50.334
the number of features.

00:00:50.335 --> 00:00:53.579
If you look at your PCA model and put a dot tab,

00:00:53.579 --> 00:00:55.983
you can see a bunch of things that are available.

00:00:55.984 --> 00:00:59.545
Let's take a look at what some of these things do for our model.

00:00:59.545 --> 00:01:01.325
So, in the notebook below,

00:01:01.325 --> 00:01:03.710
you'll see there's the scree_plot function.

00:01:03.710 --> 00:01:08.500
This function plots the amount of variance explained by each component.

00:01:08.500 --> 00:01:10.234
Let's see what this looks like.

00:01:10.234 --> 00:01:12.715
Notice that we have 15 bars,

00:01:12.715 --> 00:01:15.890
each associated with one of our principle components.

00:01:15.890 --> 00:01:17.219
If you remember, up here,

00:01:17.219 --> 00:01:20.250
we fit 15 principal components.

00:01:20.250 --> 00:01:24.109
Then, the height of the bar is the amount of variability from

00:01:24.109 --> 00:01:27.814
the original feature set that's explained by that component.

00:01:27.814 --> 00:01:29.483
So, for our first component,

00:01:29.483 --> 00:01:32.598
six percent of the available variability

00:01:32.599 --> 00:01:35.450
and all of the data is explained by that component.

00:01:35.450 --> 00:01:37.325
Then, for the second component,

00:01:37.325 --> 00:01:41.645
4.29 percent of the total variability is explained by that component,

00:01:41.644 --> 00:01:43.424
and so on and so forth.

00:01:43.424 --> 00:01:46.819
The total amount of variability explained by all

00:01:46.819 --> 00:01:49.949
15 of the first component is about 35 percent,

00:01:49.950 --> 00:01:51.620
which you can see by this line.

00:01:51.620 --> 00:01:55.490
These principal components can help you understand how many you want to

00:01:55.489 --> 00:01:59.509
keep because of how much variance in the original data as I explained.

00:01:59.510 --> 00:02:04.385
This is one way to create a metric for an unsupervised algorithm.

00:02:04.385 --> 00:02:07.609
This is one way to understand how well

00:02:07.609 --> 00:02:12.574
unsupervised algorithm is doing at predicting what's happening with your data.

00:02:12.574 --> 00:02:15.169
The other thing that we should look at is how

00:02:15.169 --> 00:02:18.655
these components relate back to the original images.

00:02:18.655 --> 00:02:22.925
The next function that you see in the notebook is called plot_component.

00:02:22.925 --> 00:02:25.850
What this does is it highlights the pixels

00:02:25.849 --> 00:02:28.894
in the image that are important for each component.

00:02:28.895 --> 00:02:30.995
So, if we look at the first component,

00:02:30.995 --> 00:02:33.500
the zero's component using this function,

00:02:33.500 --> 00:02:37.544
you can see that it's probably picking out zeros from your dataset.

00:02:37.544 --> 00:02:39.844
If we look at other components,

00:02:39.844 --> 00:02:43.864
you can see that this is picking out some other patterns in the images

00:02:43.864 --> 00:02:48.400
that are related to being able to differentiate them from one another.

00:02:48.400 --> 00:02:51.650
Essentially, these bright yellow pixels or

00:02:51.650 --> 00:02:55.685
heavy weights and these darker blue pixels are lighter weights

00:02:55.685 --> 00:02:59.585
associated with how much that pixel matters

00:02:59.585 --> 00:03:03.070
for this particular component. Now, it's your turn.


WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:04.033
Agora sabemos como obter uma saída
de uma rede neural simples,

00:00:04.067 --> 00:00:05.434
como a mostrada aqui.

00:00:05.467 --> 00:00:08.167
Queremos usar a saída
para fazer previsões.

00:00:08.200 --> 00:00:11.100
Mas como construir a rede
para fazer previsões

00:00:11.133 --> 00:00:13.834
sem saber os pesos certos
antes de começar?

00:00:13.868 --> 00:00:17.200
Podemos apresentar dados
que sabemos ser verdadeiros

00:00:17.234 --> 00:00:21.467
e colocar os parâmetros, os pesos,
para se adequarem a esses dados.

00:00:21.501 --> 00:00:25.300
Primeiro precisamos medir
o quanto as previsões estão ruins.

00:00:25.334 --> 00:00:29.567
A escolha óbvia é usar a diferença
entre o valor que queremos, y,

00:00:29.601 --> 00:00:31.934
e a saída da rede, y chapéu.

00:00:31.968 --> 00:00:34.334
Mas se a previsão
for muito alta,

00:00:34.367 --> 00:00:36.033
o erro vai ser negativo,

00:00:36.067 --> 00:00:40.033
e se a previsão for muito baixa,
o erro vai ser positivo.

00:00:40.067 --> 00:00:43.067
Preferimos tratar os erros
da mesma forma.

00:00:43.100 --> 00:00:46.701
Para que ambos sejam positivos,
vamos elevar ao quadrado.

00:00:46.734 --> 00:00:49.901
Você pode se perguntar
por que não usar o módulo.

00:00:49.934 --> 00:00:53.801
Uma característica do quadrado
é que penaliza pontos extremos

00:00:53.834 --> 00:00:55.567
mais do que pequenos erros.

00:00:55.601 --> 00:00:59.601
Elevar ao quadrado também torna
a Matemática bonita mais tarde.

00:00:59.634 --> 00:01:02.767
Este é o erro
de apenas uma previsão.

00:01:02.801 --> 00:01:06.567
Queremos saber o erro
de todo o conjunto de dados.

00:01:06.601 --> 00:01:10.000
Então vamos somar os erros
de cada registro dos dados,

00:01:10.033 --> 00:01:12.434
escrito como
o somatório sobre mu.

00:01:12.467 --> 00:01:14.734
É esta letra aqui.

00:01:14.767 --> 00:01:19.601
Agora temos o erro total da rede
sobre o conjunto inteiro.

00:01:19.634 --> 00:01:22.400
Finalmente, vamos colocar
um meio na frente,

00:01:22.434 --> 00:01:25.133
porque a Matemática
fica mais limpa depois.

00:01:25.167 --> 00:01:28.701
Essa formulação se chama
soma dos erros quadrados.

00:01:28.734 --> 00:01:30.434
Chamamos de SEQ.

00:01:30.467 --> 00:01:35.300
Ela é a soma
dos erros quadrados.

00:01:35.334 --> 00:01:38.000
Lembre que y chapéu
é a combinação linear

00:01:38.033 --> 00:01:39.501
dos pesos e entradas

00:01:39.534 --> 00:01:41.567
passada pela
função de ativação.

00:01:41.601 --> 00:01:43.234
Podemos substituir aqui

00:01:43.267 --> 00:01:46.634
e vemos que o erro
depende dos pesos wi

00:01:46.667 --> 00:01:49.234
e das entradas xi.

00:01:49.267 --> 00:01:53.634
Como disse antes, os registros
são chamados pela letra grega mu.

00:01:53.667 --> 00:01:58.834
Pense dos dados como tabelas,
vetores, matrizes, o que quiser.

00:01:58.868 --> 00:02:01.267
Uma contém
os dados de entrada, x,

00:02:01.300 --> 00:02:04.100
e a outra contém
os objetivos, y.

00:02:04.133 --> 00:02:08.701
Cada registro é uma linha,
então mu=1 é a primeira linha.

00:02:08.734 --> 00:02:13.334
Para calcular o erro total,
você passa pelas linhas da matriz

00:02:13.367 --> 00:02:15.501
e calcula a SEQ.

00:02:15.534 --> 00:02:18.367
Depois soma
todos esses resultados.

00:02:18.400 --> 00:02:21.300
A SEQ é uma medida
do desempenho da rede.

00:02:21.334 --> 00:02:24.434
Se for alto, a rede está
fazendo previsões ruins.

00:02:24.467 --> 00:02:27.567
Se for baixo, a rede está
fazendo boas previsões.

00:02:27.601 --> 00:02:30.501
Queremos torná-lo
tão pequeno quanto possível.

00:02:30.534 --> 00:02:33.200
Agora vamos considerar
um exemplo simples

00:02:33.234 --> 00:02:34.801
com só um registro

00:02:34.834 --> 00:02:38.567
para ficar mais fácil entender
como vamos minimizar o erro.

00:02:38.601 --> 00:02:42.501
Para esta rede simples, a SEQ
é o objetivo menos a previsão,

00:02:42.534 --> 00:02:46.334
y menos y chapéu, ao quadrado,
dividido por 2.

00:02:46.367 --> 00:02:48.334
Substituindo a previsão,

00:02:48.367 --> 00:02:51.200
vemos que o erro
é uma função dos pesos.

00:02:51.234 --> 00:02:54.968
Os erros podem ser alterados
para mudar a previsão da rede.

00:02:55.000 --> 00:02:57.667
Isso por sua vez
afeta o erro total.

00:02:57.701 --> 00:03:01.701
O objetivo é encontrar pesos
que minimizam o erro.

00:03:01.734 --> 00:03:05.300
Aqui está uma figura simples
do erro com um peso.

00:03:05.334 --> 00:03:08.501
O objetivo é encontrar o peso
no fundo deste pote.

00:03:08.534 --> 00:03:10.534
Começando com
um peso aleatório,

00:03:10.567 --> 00:03:13.868
queremos dar um passo
na direção do mínimo.

00:03:13.901 --> 00:03:16.667
Esta direção
é o oposto do gradiente,

00:03:16.701 --> 00:03:17.767
a inclinação.

00:03:17.801 --> 00:03:19.367
Se dermos muitos passos,

00:03:19.400 --> 00:03:21.334
sempre descendo
pelo gradiente,

00:03:21.367 --> 00:03:24.968
o peso vai acabar encontrando
o mínimo da função de erro.

00:03:25.000 --> 00:03:27.634
Esta é
a descida do gradiente.

00:03:27.667 --> 00:03:31.167
Queremos atualizar os pesos
para que o novo peso, wi,

00:03:31.200 --> 00:03:34.267
seja o antigo peso wi
mais este passo do peso,

00:03:34.300 --> 00:03:35.968
delta wi.

00:03:36.000 --> 00:03:39.767
Esta letra grega, delta,
costuma significar mudança.

00:03:39.801 --> 00:03:42.567
O passo do peso
é proporcional ao gradiente,

00:03:42.601 --> 00:03:47.234
a derivada parcial do erro
em relação a cada peso wi.

00:03:47.267 --> 00:03:50.033
Podemos colocar
um parâmetro multiplicativo

00:03:50.067 --> 00:03:53.734
para definir o tamanho dos passos
da descida do gradiente.

00:03:53.767 --> 00:03:57.634
Ele se chama taxa de aprendizagem
e usamos a letra grega eta.

00:03:57.667 --> 00:04:01.267
Calcular o gradiente requer
cálculo de várias variáveis,

00:04:01.300 --> 00:04:04.968
que você pode já saber,
porque são derivadas parciais.

00:04:05.000 --> 00:04:08.667
Não se preocupe demais se
não entender o que eu faço aqui.

00:04:08.701 --> 00:04:12.567
É mais importante entender
o conceito da descida do gradiente

00:04:12.601 --> 00:04:14.000
e o resultado final.

00:04:14.033 --> 00:04:15.534
Se precisar se lembrar,

00:04:15.567 --> 00:04:18.400
sugiro as aulas de cálculo
da Khan Academy,

00:04:18.434 --> 00:04:20.834
e vou colocar o link
para vocês.

00:04:20.868 --> 00:04:24.033
Agora para o gradiente,
tomamos a derivada parcial

00:04:24.067 --> 00:04:26.734
em relação aos pesos
do erro quadrado.

00:04:26.767 --> 00:04:30.067
A saída da rede, y chapéu,
é uma função dos pesos.

00:04:30.100 --> 00:04:33.567
O que temos aqui
é uma função de outra função

00:04:33.601 --> 00:04:35.501
que depende dos pesos.

00:04:35.534 --> 00:04:39.934
Isso requer a regra da cadeia
para calcular a derivada.

00:04:39.968 --> 00:04:42.834
Vamos lembrar
da regra da cadeia.

00:04:42.868 --> 00:04:45.434
Se quisermos a derivada
de uma função p

00:04:45.467 --> 00:04:46.901
em relação a z.

00:04:46.934 --> 00:04:50.901
Se p depende de outra função q
que depende de z,

00:04:50.934 --> 00:04:55.000
a regra da cadeia diz que primeiro
derivamos p em relação a q

00:04:55.033 --> 00:04:58.767
e multiplicamos pela derivada de q
em relação a z.

00:04:58.801 --> 00:05:02.033
Gosto de pensar nisso
como frações normais.

00:05:02.067 --> 00:05:05.300
O dq no denominador
cancela o do numerador

00:05:05.334 --> 00:05:07.634
e ficamos com dp/dz.

00:05:07.667 --> 00:05:11.400
No nosso problema,
o q é o erro,

00:05:11.434 --> 00:05:12.868
y menos y chapéu,

00:05:12.901 --> 00:05:14.834
e p é o erro quadrado.

00:05:14.868 --> 00:05:18.901
Estamos derivando
em relação a wi.

00:05:18.934 --> 00:05:23.200
A derivada de p em relação a q
dá o próprio erro.

00:05:23.234 --> 00:05:27.200
O 2 do expoente desce
e cancela com o meio.

00:05:27.234 --> 00:05:31.901
Resta a derivada do erro
em relação a wi.

00:05:31.934 --> 00:05:34.834
O valor desejado, y,
não depende dos pesos.

00:05:34.868 --> 00:05:36.701
Mas y chapéu depende.

00:05:36.734 --> 00:05:40.200
Usando a regra da cadeia de novo,
o sinal de menos sai,

00:05:40.234 --> 00:05:43.100
e sobra a derivada parcial
de y chapéu.

00:05:43.133 --> 00:05:47.367
Lembre que y chapéu
é a função de ativação em h,

00:05:47.400 --> 00:05:51.501
onde h é a combinação linear
dos pesos e entradas.

00:05:51.534 --> 00:05:55.334
Derivando y chapéu, usando
a regra da cadeia de novo,

00:05:55.367 --> 00:05:58.701
temos a derivada
da função de ativação em h

00:05:58.734 --> 00:06:01.701
vezes a derivada parcial
da combinação linear.

00:06:02.200 --> 00:06:06.100
Neste somatório, só tem um termo
que depende de cada peso.

00:06:06.133 --> 00:06:07.834
Escrevendo para o peso 1,

00:06:07.868 --> 00:06:12.167
só o primeiro termo, com x1,
depende do peso 1.

00:06:12.200 --> 00:06:17.033
A derivada parcial da soma
em relação ao peso 1 é só x1.

00:06:17.067 --> 00:06:19.300
Todos os outros termos
são zero.

00:06:19.334 --> 00:06:22.701
Então a derivada parcial
do somatório em relação a wi

00:06:22.734 --> 00:06:24.601
é só xi.

00:06:24.634 --> 00:06:28.267
Finalmente, juntando tudo,
o gradiente do erro quadrado

00:06:28.300 --> 00:06:31.300
em relação a wi é:
menos o erro,

00:06:31.334 --> 00:06:34.767
vezes a derivada
da função de ativação em h,

00:06:34.801 --> 00:06:36.868
vezes a entrada xi.

00:06:36.901 --> 00:06:39.801
O passo do peso é
a velocidade de treinamento,

00:06:39.834 --> 00:06:42.534
vezes o erro,
vezes a derivada da ativação,

00:06:42.567 --> 00:06:44.067
vezes a entrada.

00:06:44.100 --> 00:06:46.734
Por conveniência,
e para facilitar depois,

00:06:46.767 --> 00:06:48.834
podemos definir
um termo de erro,

00:06:48.868 --> 00:06:49.901
delta minúsculo,

00:06:49.934 --> 00:06:53.634
como o erro vezes a derivada
da função de ativação em h.

00:06:53.667 --> 00:06:56.067
Então escrevemos
a atualização do peso

00:06:56.100 --> 00:06:59.567
como o que era antes vezes
a velocidade de treinamento,

00:06:59.601 --> 00:07:02.133
vezes o termo de erro,
vezes xi.

00:07:02.167 --> 00:07:04.100
É o valor da entrada i.

00:07:04.133 --> 00:07:07.234
Você pode estar trabalhando
com múltiplas saídas.

00:07:07.267 --> 00:07:11.300
Pense que está empilhando
a arquitetura de uma saída

00:07:11.334 --> 00:07:15.334
mas conectando as entradas
à nova saída.

00:07:15.367 --> 00:07:20.434
Agora o erro total incluiria
o erro de todas as saídas somado.

00:07:20.467 --> 00:07:23.033
A descida do gradiente
pode ser estendida

00:07:23.067 --> 00:07:25.200
para uma rede
com múltiplas saídas

00:07:25.234 --> 00:07:28.000
calculando um termo de erro
para cada saída,

00:07:28.033 --> 00:07:30.267
denotado com o subscrito j.

00:07:30.300 --> 00:07:32.934
Vou mostrar como
traduzir isto para código

00:07:32.968 --> 00:07:35.434
para implementar
em Python e NumPy.


WEBVTT
Kind: captions
Language: en

00:00:00.750 --> 00:00:05.030
Now we know how to get an output from a
simple neural network like the one shown

00:00:05.030 --> 00:00:05.910
here.

00:00:05.910 --> 00:00:09.690
We'd like to use the output to make
predictions, but how do we build this

00:00:09.690 --> 00:00:14.030
network to make predictions without
knowing the correct weights before hand?

00:00:14.030 --> 00:00:17.530
What we can do is present it with
data that we know to be true

00:00:17.530 --> 00:00:21.640
then set the model parameters,
the weights to match that data.

00:00:21.640 --> 00:00:25.330
First we need some measure of
how bad our predictions are.

00:00:25.330 --> 00:00:28.500
The obvious choice is to use
the difference between the true target

00:00:28.500 --> 00:00:32.220
value, y, and the network output, y hat.

00:00:32.220 --> 00:00:36.090
However, if the prediction is too high,
this error will be negative and

00:00:36.090 --> 00:00:40.270
if their prediction is too low by the
same amount the error will be positive.

00:00:40.270 --> 00:00:42.370
We'd rather treat these
errors the same to

00:00:43.420 --> 00:00:46.890
make both cases positive
we'll just square the error.

00:00:46.890 --> 00:00:50.630
You might be wondering why not
just take the absolute value.

00:00:50.630 --> 00:00:53.250
One bit of using the square
is that it penalizes

00:00:53.250 --> 00:00:56.020
outliers more than small errors.

00:00:56.020 --> 00:00:59.690
Also squaring the error
makes the math nice later.

00:00:59.690 --> 00:01:02.455
This is the error for
just one prediction though.

00:01:02.455 --> 00:01:06.860
We'd rather like to know the error for
the entire dataset.

00:01:06.860 --> 00:01:12.000
So we'll just sum up the errors for each
data record denoted by the sum over mu,

00:01:13.010 --> 00:01:15.230
this u looking guy here.

00:01:15.230 --> 00:01:20.190
Now we have the total error for
the network over the entire dataset.

00:01:20.190 --> 00:01:20.990
And finally,

00:01:20.990 --> 00:01:25.100
we'll add a one half in front
because it cleans up the math later.

00:01:25.100 --> 00:01:29.010
This formulation is typically called
the sum of the squared errors.

00:01:29.010 --> 00:01:34.550
SSE for short because well it is
the sum of the squared errors.

00:01:35.590 --> 00:01:38.910
Remember that y hat is the linear
combination of the weights and

00:01:38.910 --> 00:01:41.780
inputs passed through
that activation function.

00:01:41.780 --> 00:01:45.240
We can substitute it in here,
then we see that the error depends on

00:01:45.240 --> 00:01:48.500
the weights, wi, and
the input values, xi.

00:01:49.580 --> 00:01:53.850
As I said before the data records
are represented by the Greek letter mu.

00:01:53.850 --> 00:01:57.360
You can think of the data as two
tables or arrays, or matrices,

00:01:57.360 --> 00:01:59.180
whatever works for you.

00:01:59.180 --> 00:02:04.360
One contains the input data, x, and
the other contains the targets, y.

00:02:04.360 --> 00:02:07.940
Each record is one row here, so
mu equals 1 is the first row.

00:02:08.940 --> 00:02:11.160
Then, to calculate the total error,

00:02:11.160 --> 00:02:15.590
you're just scanning through the rows of
these arrays and calculating the SSE.

00:02:15.590 --> 00:02:17.440
Then summing up all of those results.

00:02:18.580 --> 00:02:21.800
The SSE is a measure of
our network's performance.

00:02:21.800 --> 00:02:24.540
If it's high,
the network is making bad predictions.

00:02:24.540 --> 00:02:27.800
If it's low,
the network is making good predictions.

00:02:27.800 --> 00:02:30.700
So we want to make it
as small as possible.

00:02:30.700 --> 00:02:35.220
Going forward, let's consider a simple
example with only one data record

00:02:35.220 --> 00:02:38.620
to make it easier to understand
how we'll minimize the error.

00:02:38.620 --> 00:02:43.530
For the simple network the SSE is
the true target minus the prediction,

00:02:43.530 --> 00:02:46.680
y- y hat and squared and divided by 2.

00:02:46.680 --> 00:02:51.440
Substituting for the prediction, you see
the error is a function of the weights.

00:02:51.440 --> 00:02:54.880
The weights are the knobs we can use
to alter the network's predictions

00:02:54.880 --> 00:02:57.930
which in turn affects the overall error.

00:02:57.930 --> 00:03:00.919
Then our goal is to find weights
that minimize the error.

00:03:01.950 --> 00:03:05.580
Here is a simple depiction of
the error with one weight.

00:03:05.580 --> 00:03:08.790
Our goals is to find the weight
at the bottom of this bowl.

00:03:08.790 --> 00:03:10.340
Starting at some random weight,

00:03:10.340 --> 00:03:14.130
we want to make a step in
the direction towards the minimum.

00:03:14.130 --> 00:03:18.190
This direction is the opposite
to the gradient, the slope.

00:03:18.190 --> 00:03:21.580
If we take many steps,
always descending down a gradient.

00:03:21.580 --> 00:03:25.410
Eventually the weight will find
the minimum of the error function and

00:03:25.410 --> 00:03:27.900
this is gradient descent.

00:03:27.900 --> 00:03:32.490
We want to update the weight, so
a new weight, wi, is the old weight,

00:03:32.490 --> 00:03:34.940
wi plus this weight step, delta wi.

00:03:36.140 --> 00:03:40.070
This Greek letter, delta,
typically signifies change.

00:03:40.070 --> 00:03:43.169
The weight step is
proportional to the gradient,

00:03:43.169 --> 00:03:47.543
the partial derivative of the error
with respect to each weight, wi.

00:03:47.543 --> 00:03:51.367
We can add in an arbitrary scaling
parameter that allows us to set the size

00:03:51.367 --> 00:03:53.780
of the gradient descent steps.

00:03:53.780 --> 00:03:56.720
This is called the learning rate,
signified by the Greek letter eta.

00:03:57.910 --> 00:04:01.410
Calculating the gradient here
requires multivariable calculus.

00:04:01.410 --> 00:04:04.970
Which you might already know since
we're taking partial derivatives.

00:04:04.970 --> 00:04:08.890
I wouldn't worry too much if you
don't understand what I'm doing here.

00:04:08.890 --> 00:04:12.540
It's more important to understand
the concept of gradient descent and

00:04:12.540 --> 00:04:14.380
the final result.

00:04:14.380 --> 00:04:18.620
If you need a refresher, I suggest Khan
Academy multivariable calculus lessons,

00:04:18.620 --> 00:04:21.130
which I'll link to for your convenience.

00:04:21.130 --> 00:04:24.030
Writing out the gradient,
you get the partial derivative

00:04:24.030 --> 00:04:26.970
with respect to the weights
of the squared error.

00:04:26.970 --> 00:04:30.330
The network output, y hat,
is a function of the weights.

00:04:30.330 --> 00:04:33.820
So what we have here is
a function of another function

00:04:33.820 --> 00:04:35.750
that depends on the weights.

00:04:35.750 --> 00:04:39.080
This requires using the chain
rule to calculate the derivative.

00:04:40.210 --> 00:04:43.120
Here is a quick refresher
on the chain rule.

00:04:43.120 --> 00:04:46.340
Say you want to take the derivative
of a function p with respect to z.

00:04:46.340 --> 00:04:51.150
If p depends on another
function q that depends on z.

00:04:51.150 --> 00:04:55.442
The chain rule says, you first take the
derivative of p with the respect to q,

00:04:55.442 --> 00:04:58.825
then multiply it by the derivative
of q with the respect to z.

00:04:58.825 --> 00:05:02.879
I like to envision this as
normal fractions, the d q's and

00:05:02.879 --> 00:05:07.535
the denominator and numerator
cancel out, and you get back dpdz.

00:05:07.535 --> 00:05:11.877
This relates to our problem because
we can set q to the error,

00:05:11.877 --> 00:05:14.811
y- y hat and set p to the squared error.

00:05:14.811 --> 00:05:18.842
And then we're taking
the derivative with respect to wi,

00:05:18.842 --> 00:05:23.211
the derivative of p with respect
to q returns the error itself.

00:05:23.211 --> 00:05:27.100
The 2 in the exponent drops down and
cancels out the 1/2.

00:05:27.100 --> 00:05:31.140
Then we're left with the derivative
of the error with respect to wi.

00:05:32.290 --> 00:05:36.120
The target value y doesn't depend
on the weights, but y hat does.

00:05:37.130 --> 00:05:40.200
Using the Chain Rule again,
the minus sign comes out in front and

00:05:40.200 --> 00:05:43.350
we're left with the partial
derivative of y hat.

00:05:43.350 --> 00:05:47.650
If you remember, y hat is equal
to the activation function at h.

00:05:47.650 --> 00:05:51.810
Where h is the linear combination
of the weights and input values.

00:05:51.810 --> 00:05:55.520
Taking the derivative of y hat,
and again using the chain rule.

00:05:55.520 --> 00:05:59.010
You get the derivative of
the activation function at h,

00:05:59.010 --> 00:06:02.390
times the partial derivative
of the linear combination.

00:06:02.390 --> 00:06:06.310
In the sum, there is only one
term that depends on each weight.

00:06:06.310 --> 00:06:07.090
Writing this out for

00:06:07.090 --> 00:06:12.410
weight one, you see that only the first
term with x1 depends on weight one.

00:06:12.410 --> 00:06:17.180
Then the partial derivative of the sum
with respect to weight one is just x1.

00:06:17.180 --> 00:06:19.520
All the other terms are zero,

00:06:19.520 --> 00:06:24.860
then the partial derivative of this
sum with respect to wi is just xi.

00:06:24.860 --> 00:06:28.420
Finally, putting all this together,
the gradient of the squared error

00:06:28.420 --> 00:06:31.540
with respect to wi is
the negative of the error

00:06:31.540 --> 00:06:37.410
times the derivative of the activation
function at h times the input value xi.

00:06:37.410 --> 00:06:40.820
Then the weight step is a learning
rate eta times the error,

00:06:40.820 --> 00:06:44.590
times the activation derivative,
times the input value.

00:06:44.590 --> 00:06:48.730
For convenience, and to make things
easy later, we can define an error term,

00:06:48.730 --> 00:06:53.250
lowercase delta, as the error times
the activation function derivative at h.

00:06:54.360 --> 00:06:58.660
Then we can write our weight
update as wi equals wi

00:06:58.660 --> 00:07:03.320
plus the learning rate times the error
term times xi is the value of input i.

00:07:04.350 --> 00:07:07.410
You might be working with
multiple output units.

00:07:07.410 --> 00:07:10.820
You can think of this as just stacking
the architecture from the single output

00:07:10.820 --> 00:07:14.469
network but connecting the input
units to the new output units.

00:07:15.820 --> 00:07:19.940
Now the total error would include
the error of each outputs sum together

00:07:20.770 --> 00:07:25.030
The gradient descent step can be extended 
to a network with multiple output.

00:07:25.030 --> 00:07:27.148
By calculating an error term for

00:07:27.148 --> 00:07:30.213
each output unit denoted
with the subscript j.

00:07:30.213 --> 00:07:33.000
Next, I'll show you how to
translate this into codes.

00:07:33.000 --> 00:07:35.267
So you can implement gradient descent in Python and Numpy


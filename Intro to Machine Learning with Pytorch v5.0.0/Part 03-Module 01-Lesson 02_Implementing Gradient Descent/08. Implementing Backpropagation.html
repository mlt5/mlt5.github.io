<!-- udacity2.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Implementing Backpropagation</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Implementing Gradient Descent</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Mean Squared Error Function.html">01. Mean Squared Error Function</a>
    </li>
    <li class="">
      <a href="02. Gradient Descent.html">02. Gradient Descent</a>
    </li>
    <li class="">
      <a href="03. Gradient Descent The Math.html">03. Gradient Descent: The Math</a>
    </li>
    <li class="">
      <a href="04. Gradient Descent The Code.html">04. Gradient Descent: The Code</a>
    </li>
    <li class="">
      <a href="05. Implementing Gradient Descent.html">05. Implementing Gradient Descent</a>
    </li>
    <li class="">
      <a href="06. Multilayer Perceptrons.html">06. Multilayer Perceptrons</a>
    </li>
    <li class="">
      <a href="07. Backpropagation.html">07. Backpropagation</a>
    </li>
    <li class="">
      <a href="08. Implementing Backpropagation.html">08. Implementing Backpropagation</a>
    </li>
    <li class="">
      <a href="09. Further Reading.html">09. Further Reading</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">08. Implementing Backpropagation</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="implementing-backpropagation">Implementing backpropagation</h1>
<p>Now we've seen that the error term for the output layer is</p>
<p><span class="mathquill ud-math">\delta_k = (y_k - \hat y_k) f'(a_k)</span></p>
<p>and the error term for the hidden layer is</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/hidden-errors.gif" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>For now we'll only consider a simple network with one hidden layer and one output unit. Here's the general algorithm for updating the weights with backpropagation:</p>
<ul>
<li><p>Set the weight steps for each layer to zero</p>
<ul>
<li>The input to hidden weights <span class="mathquill ud-math">\Delta w_{ij} = 0</span></li>
<li>The hidden to output weights <span class="mathquill ud-math">\Delta W_j = 0</span></li></ul></li>
<li><p>For each record in the training data: </p>
<ul>
<li><p>Make a forward pass through the network, calculating the output <span class="mathquill ud-math">\hat y</span></p></li>
<li><p>Calculate the error gradient in the output unit, <span class="mathquill ud-math">\delta^o = (y - \hat y)  f'(z)</span> where <span class="mathquill ud-math">z = \sum_j W_j a_j</span>, the input to the output unit.</p></li>
<li><p>Propagate the errors to the hidden layer <span class="mathquill ud-math">\delta^h_j = \delta^o W_j  f'(h_j)</span></p></li>
<li><p>Update the weight steps:</p>
<ul>
<li><span class="mathquill ud-math">\Delta W_j = \Delta W_j + \delta^o a_j</span></li>
<li><span class="mathquill ud-math">\Delta w_{ij} = \Delta w_{ij} + \delta^h_j a_i </span></li></ul></li></ul></li>
<li><p>Update the weights, where <span class="mathquill ud-math">\eta</span> is the learning rate and <span class="mathquill ud-math">m</span> is the number of records:</p>
<ul>
<li><span class="mathquill ud-math"> W_j = W_j  + \eta \Delta W_j / m</span></li>
<li><span class="mathquill ud-math"> w_{ij} = w_{ij}  + \eta \Delta w_{ij} / m</span></li></ul></li>
<li><p>Repeat for <span class="mathquill ud-math">e</span> epochs.</p></li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="backpropagation-exercise">Backpropagation exercise</h2>
<p>Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.</p>
<p>Your goals here:</p>
<ul>
<li>Implement the forward pass.</li>
<li>Implement the backpropagation algorithm.</li>
<li>Update the weights.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#258884-backprop-py" class="nav-item nav-link  active show" id="tab-258884-backprop-py" data-toggle="tab" role="tab"
      aria-controls="258884-backprop-py" aria-selected="true">backprop.py</a>
    <a href="#258884-data_prep-py" class="nav-item nav-link " id="tab-258884-data_prep-py" data-toggle="tab" role="tab"
      aria-controls="258884-data_prep-py" aria-selected="false">data_prep.py</a>
    <a href="#258884-binary-csv" class="nav-item nav-link " id="tab-258884-binary-csv" data-toggle="tab" role="tab"
      aria-controls="258884-binary-csv" aria-selected="false">binary.csv</a>
    <a href="#258884-solution-py" class="nav-item nav-link " id="tab-258884-solution-py" data-toggle="tab" role="tab"
      aria-controls="258884-solution-py" aria-selected="false">solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="258884-backprop-py" aria-labelledby="tab-258884-backprop-py" role="tabpanel">
      <pre><code></code>import numpy as np
from data_prep import features, targets, features_test, targets_test

np.random.seed(21)

def sigmoid(x):
    &quot;&quot;&quot;
    Calculate sigmoid
    &quot;&quot;&quot;
    return 1 / (1 + np.exp(-x))


# Hyperparameters
n_hidden &#x3D; 2  # number of hidden units
epochs &#x3D; 900
learnrate &#x3D; 0.005

n_records, n_features &#x3D; features.shape
last_loss &#x3D; None
# Initialize weights
weights_input_hidden &#x3D; np.random.normal(scale&#x3D;1 / n_features ** .5,
                                        size&#x3D;(n_features, n_hidden))
weights_hidden_output &#x3D; np.random.normal(scale&#x3D;1 / n_features ** .5,
                                         size&#x3D;n_hidden)

for e in range(epochs):
    del_w_input_hidden &#x3D; np.zeros(weights_input_hidden.shape)
    del_w_hidden_output &#x3D; np.zeros(weights_hidden_output.shape)
    for x, y in zip(features.values, targets):
        ## Forward pass ##
        # TODO: Calculate the output
        hidden_input &#x3D; None
        hidden_output &#x3D; None
        output &#x3D; None

        ## Backward pass ##
        # TODO: Calculate the network&#x27;s prediction error
        error &#x3D; None

        # TODO: Calculate error term for the output unit
        output_error_term &#x3D; None

        ## propagate errors to hidden layer

        # TODO: Calculate the hidden layer&#x27;s contribution to the error
        hidden_error &#x3D; None
        
        # TODO: Calculate the error term for the hidden layer
        hidden_error_term &#x3D; None
        
        # TODO: Update the change in weights
        del_w_hidden_output +&#x3D; 0
        del_w_input_hidden +&#x3D; 0

    # TODO: Update weights  (don&#x27;t forget to division by n_records or number of samples)
    weights_input_hidden +&#x3D; 0
    weights_hidden_output +&#x3D; 0

    # Printing out the mean square error on the training set
    if e % (epochs / 10) &#x3D;&#x3D; 0:
        hidden_output &#x3D; sigmoid(np.dot(x, weights_input_hidden))
        out &#x3D; sigmoid(np.dot(hidden_output,
                             weights_hidden_output))
        loss &#x3D; np.mean((out - targets) ** 2)

        if last_loss and last_loss &lt; loss:
            print(&quot;Train loss: &quot;, loss, &quot;  WARNING - Loss Increasing&quot;)
        else:
            print(&quot;Train loss: &quot;, loss)
        last_loss &#x3D; loss

# Calculate accuracy on test data
hidden &#x3D; sigmoid(np.dot(features_test, weights_input_hidden))
out &#x3D; sigmoid(np.dot(hidden, weights_hidden_output))
predictions &#x3D; out &gt; 0.5
accuracy &#x3D; np.mean(predictions &#x3D;&#x3D; targets_test)
print(&quot;Prediction accuracy: {:.3f}&quot;.format(accuracy))
</code></pre>
    </div>
    <div class="tab-pane " id="258884-data_prep-py" aria-labelledby="tab-258884-data_prep-py" role="tabpanel">
      <pre><code></code>import numpy as np
import pandas as pd

admissions &#x3D; pd.read_csv(&#x27;binary.csv&#x27;)

# Make dummy variables for rank
data &#x3D; pd.concat([admissions, pd.get_dummies(admissions[&#x27;rank&#x27;], prefix&#x3D;&#x27;rank&#x27;)], axis&#x3D;1)
data &#x3D; data.drop(&#x27;rank&#x27;, axis&#x3D;1)

# Standarize features
for field in [&#x27;gre&#x27;, &#x27;gpa&#x27;]:
    mean, std &#x3D; data[field].mean(), data[field].std()
    data.loc[:,field] &#x3D; (data[field]-mean)/std
    
# Split off random 10% of the data for testing
np.random.seed(21)
sample &#x3D; np.random.choice(data.index, size&#x3D;int(len(data)*0.9), replace&#x3D;False)
data, test_data &#x3D; data.ix[sample], data.drop(sample)

# Split into features and targets
features, targets &#x3D; data.drop(&#x27;admit&#x27;, axis&#x3D;1), data[&#x27;admit&#x27;]
features_test, targets_test &#x3D; test_data.drop(&#x27;admit&#x27;, axis&#x3D;1), test_data[&#x27;admit&#x27;]</code></pre>
    </div>
    <div class="tab-pane " id="258884-binary-csv" aria-labelledby="tab-258884-binary-csv" role="tabpanel">
      <pre><code></code>admit,gre,gpa,rank
0,380,3.61,3
1,660,3.67,3
1,800,4,1
1,640,3.19,4
0,520,2.93,4
1,760,3,2
1,560,2.98,1
0,400,3.08,2
1,540,3.39,3
0,700,3.92,2
0,800,4,4
0,440,3.22,1
1,760,4,1
0,700,3.08,2
1,700,4,1
0,480,3.44,3
0,780,3.87,4
0,360,2.56,3
0,800,3.75,2
1,540,3.81,1
0,500,3.17,3
1,660,3.63,2
0,600,2.82,4
0,680,3.19,4
1,760,3.35,2
1,800,3.66,1
1,620,3.61,1
1,520,3.74,4
1,780,3.22,2
0,520,3.29,1
0,540,3.78,4
0,760,3.35,3
0,600,3.4,3
1,800,4,3
0,360,3.14,1
0,400,3.05,2
0,580,3.25,1
0,520,2.9,3
1,500,3.13,2
1,520,2.68,3
0,560,2.42,2
1,580,3.32,2
1,600,3.15,2
0,500,3.31,3
0,700,2.94,2
1,460,3.45,3
1,580,3.46,2
0,500,2.97,4
0,440,2.48,4
0,400,3.35,3
0,640,3.86,3
0,440,3.13,4
0,740,3.37,4
1,680,3.27,2
0,660,3.34,3
1,740,4,3
0,560,3.19,3
0,380,2.94,3
0,400,3.65,2
0,600,2.82,4
1,620,3.18,2
0,560,3.32,4
0,640,3.67,3
1,680,3.85,3
0,580,4,3
0,600,3.59,2
0,740,3.62,4
0,620,3.3,1
0,580,3.69,1
0,800,3.73,1
0,640,4,3
0,300,2.92,4
0,480,3.39,4
0,580,4,2
0,720,3.45,4
0,720,4,3
0,560,3.36,3
1,800,4,3
0,540,3.12,1
1,620,4,1
0,700,2.9,4
0,620,3.07,2
0,500,2.71,2
0,380,2.91,4
1,500,3.6,3
0,520,2.98,2
0,600,3.32,2
0,600,3.48,2
0,700,3.28,1
1,660,4,2
0,700,3.83,2
1,720,3.64,1
0,800,3.9,2
0,580,2.93,2
1,660,3.44,2
0,660,3.33,2
0,640,3.52,4
0,480,3.57,2
0,700,2.88,2
0,400,3.31,3
0,340,3.15,3
0,580,3.57,3
0,380,3.33,4
0,540,3.94,3
1,660,3.95,2
1,740,2.97,2
1,700,3.56,1
0,480,3.13,2
0,400,2.93,3
0,480,3.45,2
0,680,3.08,4
0,420,3.41,4
0,360,3,3
0,600,3.22,1
0,720,3.84,3
0,620,3.99,3
1,440,3.45,2
0,700,3.72,2
1,800,3.7,1
0,340,2.92,3
1,520,3.74,2
1,480,2.67,2
0,520,2.85,3
0,500,2.98,3
0,720,3.88,3
0,540,3.38,4
1,600,3.54,1
0,740,3.74,4
0,540,3.19,2
0,460,3.15,4
1,620,3.17,2
0,640,2.79,2
0,580,3.4,2
0,500,3.08,3
0,560,2.95,2
0,500,3.57,3
0,560,3.33,4
0,700,4,3
0,620,3.4,2
1,600,3.58,1
0,640,3.93,2
1,700,3.52,4
0,620,3.94,4
0,580,3.4,3
0,580,3.4,4
0,380,3.43,3
0,480,3.4,2
0,560,2.71,3
1,480,2.91,1
0,740,3.31,1
1,800,3.74,1
0,400,3.38,2
1,640,3.94,2
0,580,3.46,3
0,620,3.69,3
1,580,2.86,4
0,560,2.52,2
1,480,3.58,1
0,660,3.49,2
0,700,3.82,3
0,600,3.13,2
0,640,3.5,2
1,700,3.56,2
0,520,2.73,2
0,580,3.3,2
0,700,4,1
0,440,3.24,4
0,720,3.77,3
0,500,4,3
0,600,3.62,3
0,400,3.51,3
0,540,2.81,3
0,680,3.48,3
1,800,3.43,2
0,500,3.53,4
1,620,3.37,2
0,520,2.62,2
1,620,3.23,3
0,620,3.33,3
0,300,3.01,3
0,620,3.78,3
0,500,3.88,4
0,700,4,2
1,540,3.84,2
0,500,2.79,4
0,800,3.6,2
0,560,3.61,3
0,580,2.88,2
0,560,3.07,2
0,500,3.35,2
1,640,2.94,2
0,800,3.54,3
0,640,3.76,3
0,380,3.59,4
1,600,3.47,2
0,560,3.59,2
0,660,3.07,3
1,400,3.23,4
0,600,3.63,3
0,580,3.77,4
0,800,3.31,3
1,580,3.2,2
1,700,4,1
0,420,3.92,4
1,600,3.89,1
1,780,3.8,3
0,740,3.54,1
1,640,3.63,1
0,540,3.16,3
0,580,3.5,2
0,740,3.34,4
0,580,3.02,2
0,460,2.87,2
0,640,3.38,3
1,600,3.56,2
1,660,2.91,3
0,340,2.9,1
1,460,3.64,1
0,460,2.98,1
1,560,3.59,2
0,540,3.28,3
0,680,3.99,3
1,480,3.02,1
0,800,3.47,3
0,800,2.9,2
1,720,3.5,3
0,620,3.58,2
0,540,3.02,4
0,480,3.43,2
1,720,3.42,2
0,580,3.29,4
0,600,3.28,3
0,380,3.38,2
0,420,2.67,3
1,800,3.53,1
0,620,3.05,2
1,660,3.49,2
0,480,4,2
0,500,2.86,4
0,700,3.45,3
0,440,2.76,2
1,520,3.81,1
1,680,2.96,3
0,620,3.22,2
0,540,3.04,1
0,800,3.91,3
0,680,3.34,2
0,440,3.17,2
0,680,3.64,3
0,640,3.73,3
0,660,3.31,4
0,620,3.21,4
1,520,4,2
1,540,3.55,4
1,740,3.52,4
0,640,3.35,3
1,520,3.3,2
1,620,3.95,3
0,520,3.51,2
0,640,3.81,2
0,680,3.11,2
0,440,3.15,2
1,520,3.19,3
1,620,3.95,3
1,520,3.9,3
0,380,3.34,3
0,560,3.24,4
1,600,3.64,3
1,680,3.46,2
0,500,2.81,3
1,640,3.95,2
0,540,3.33,3
1,680,3.67,2
0,660,3.32,1
0,520,3.12,2
1,600,2.98,2
0,460,3.77,3
1,580,3.58,1
1,680,3,4
1,660,3.14,2
0,660,3.94,2
0,360,3.27,3
0,660,3.45,4
0,520,3.1,4
1,440,3.39,2
0,600,3.31,4
1,800,3.22,1
1,660,3.7,4
0,800,3.15,4
0,420,2.26,4
1,620,3.45,2
0,800,2.78,2
0,680,3.7,2
0,800,3.97,1
0,480,2.55,1
0,520,3.25,3
0,560,3.16,1
0,460,3.07,2
0,540,3.5,2
0,720,3.4,3
0,640,3.3,2
1,660,3.6,3
1,400,3.15,2
1,680,3.98,2
0,220,2.83,3
0,580,3.46,4
1,540,3.17,1
0,580,3.51,2
0,540,3.13,2
0,440,2.98,3
0,560,4,3
0,660,3.67,2
0,660,3.77,3
1,520,3.65,4
0,540,3.46,4
1,300,2.84,2
1,340,3,2
1,780,3.63,4
1,480,3.71,4
0,540,3.28,1
0,460,3.14,3
0,460,3.58,2
0,500,3.01,4
0,420,2.69,2
0,520,2.7,3
0,680,3.9,1
0,680,3.31,2
1,560,3.48,2
0,580,3.34,2
0,500,2.93,4
0,740,4,3
0,660,3.59,3
0,420,2.96,1
0,560,3.43,3
1,460,3.64,3
1,620,3.71,1
0,520,3.15,3
0,620,3.09,4
0,540,3.2,1
1,660,3.47,3
0,500,3.23,4
1,560,2.65,3
0,500,3.95,4
0,580,3.06,2
0,520,3.35,3
0,500,3.03,3
0,600,3.35,2
0,580,3.8,2
0,400,3.36,2
0,620,2.85,2
1,780,4,2
0,620,3.43,3
1,580,3.12,3
0,700,3.52,2
1,540,3.78,2
1,760,2.81,1
0,700,3.27,2
0,720,3.31,1
1,560,3.69,3
0,720,3.94,3
1,520,4,1
1,540,3.49,1
0,680,3.14,2
0,460,3.44,2
1,560,3.36,1
0,480,2.78,3
0,460,2.93,3
0,620,3.63,3
0,580,4,1
0,800,3.89,2
1,540,3.77,2
1,680,3.76,3
1,680,2.42,1
1,620,3.37,1
0,560,3.78,2
0,560,3.49,4
0,620,3.63,2
1,800,4,2
0,640,3.12,3
0,540,2.7,2
0,700,3.65,2
1,540,3.49,2
0,540,3.51,2
0,660,4,1
1,480,2.62,2
0,420,3.02,1
1,740,3.86,2
0,580,3.36,2
0,640,3.17,2
0,640,3.51,2
1,800,3.05,2
1,660,3.88,2
1,600,3.38,3
1,620,3.75,2
1,460,3.99,3
0,620,4,2
0,560,3.04,3
0,460,2.63,2
0,700,3.65,2
0,600,3.89,3
</code></pre>
    </div>
    <div class="tab-pane " id="258884-solution-py" aria-labelledby="tab-258884-solution-py" role="tabpanel">
      <pre><code></code>import numpy as np
from data_prep import features, targets, features_test, targets_test

np.random.seed(21)

def sigmoid(x):
    &quot;&quot;&quot;
    Calculate sigmoid
    &quot;&quot;&quot;
    return 1 / (1 + np.exp(-x))


# Hyperparameters
n_hidden &#x3D; 2  # number of hidden units
epochs &#x3D; 900
learnrate &#x3D; 0.005

n_records, n_features &#x3D; features.shape
last_loss &#x3D; None
# Initialize weights
weights_input_hidden &#x3D; np.random.normal(scale&#x3D;1 / n_features ** .5,
                                        size&#x3D;(n_features, n_hidden))
weights_hidden_output &#x3D; np.random.normal(scale&#x3D;1 / n_features ** .5,
                                         size&#x3D;n_hidden)

for e in range(epochs):
    del_w_input_hidden &#x3D; np.zeros(weights_input_hidden.shape)
    del_w_hidden_output &#x3D; np.zeros(weights_hidden_output.shape)
    for x, y in zip(features.values, targets):
        ## Forward pass ##
        # TODO: Calculate the output
        hidden_input &#x3D; np.dot(x, weights_input_hidden)
        hidden_output &#x3D; sigmoid(hidden_input)

        output &#x3D; sigmoid(np.dot(hidden_output,
                                weights_hidden_output))

        ## Backward pass ##
        # TODO: Calculate the network&#x27;s prediction error
        error &#x3D; y - output

        # TODO: Calculate error term for the output unit
        output_error_term &#x3D; error * output * (1 - output)

        ## propagate errors to hidden layer

        # TODO: Calculate the hidden layer&#x27;s contribution to the error
        hidden_error &#x3D; np.dot(output_error_term, weights_hidden_output)

        # TODO: Calculate the error term for the hidden layer
        hidden_error_term &#x3D; hidden_error * hidden_output * (1 - hidden_output)

        # TODO: Update the change in weights
        del_w_hidden_output +&#x3D; output_error_term * hidden_output
        del_w_input_hidden +&#x3D; hidden_error_term * x[:, None]

    # TODO: Update weights
    weights_input_hidden +&#x3D; learnrate * del_w_input_hidden / n_records
    weights_hidden_output +&#x3D; learnrate * del_w_hidden_output / n_records

    # Printing out the mean square error on the training set
    if e % (epochs / 10) &#x3D;&#x3D; 0:
        hidden_output &#x3D; sigmoid(np.dot(x, weights_input_hidden))
        out &#x3D; sigmoid(np.dot(hidden_output,
                             weights_hidden_output))
        loss &#x3D; np.mean((out - targets) ** 2)

        if last_loss and last_loss &lt; loss:
            print(&quot;Train loss: &quot;, loss, &quot;  WARNING - Loss Increasing&quot;)
        else:
            print(&quot;Train loss: &quot;, loss)
        last_loss &#x3D; loss

# Calculate accuracy on test data
hidden &#x3D; sigmoid(np.dot(features_test, weights_input_hidden))
out &#x3D; sigmoid(np.dot(hidden, weights_hidden_output))
predictions &#x3D; out &gt; 0.5
accuracy &#x3D; np.mean(predictions &#x3D;&#x3D; targets_test)
print(&quot;Prediction accuracy: {:.3f}&quot;.format(accuracy))
</code></pre>
    </div>
  </div>
</div>



</div>


  <div class="jumbotron" style="margin-top: 50px; margin-bottom: 50px;">
    <h3>INSTRUCTOR NOTE:</h3>
    <p><strong>Note:</strong> This code takes a while to execute, so Udacity's servers sometimes return with an error saying it took too long. If that happens, it usually works if you try again. </p>
  </div>
</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="09. Further Reading.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://us-udacity.github.io/" target="_blank">【udacity2.0 】If you need more courses, please add wechat：udacity6</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('08. Implementing Backpropagation')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>

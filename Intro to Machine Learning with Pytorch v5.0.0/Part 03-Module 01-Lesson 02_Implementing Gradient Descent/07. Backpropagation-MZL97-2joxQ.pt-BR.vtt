WEBVTT
Kind: captions
Language: pt-BR

00:00:00.000 --> 00:00:02.767
Agora estamos lidando
com múltiplas camadas,

00:00:02.801 --> 00:00:06.033
e queremos treinar a rede
com descida do gradiente.

00:00:06.067 --> 00:00:08.701
Já sabemos calcular o erro
no nó de saída.

00:00:08.734 --> 00:00:11.133
Podemos usar este erro
com descida do gradiente

00:00:11.167 --> 00:00:13.400
para treinar os pesos
da oculta para a saída.

00:00:13.434 --> 00:00:15.968
Mas para treinar os pesos
da entrada para a oculta,

00:00:16.000 --> 00:00:19.367
precisamos saber o erro
das unidades da camada oculta.

00:00:19.400 --> 00:00:20.767
Como encontrar esses erros

00:00:20.801 --> 00:00:22.701
para usar no passo
da descida do gradiente?

00:00:22.734 --> 00:00:26.400
Antes encontramos os erros usando
a derivada dos erros ao quadrado

00:00:26.434 --> 00:00:29.601
em relação aos pesos entre
as camadas de entrada e saída.

00:00:29.634 --> 00:00:31.000
Se fizermos isso
com a camada oculta,

00:00:31.033 --> 00:00:32.367
usando a regra da cadeia,

00:00:32.400 --> 00:00:34.534
temos que o erro
para as suas unidades

00:00:34.567 --> 00:00:36.767
é proporcional ao erro
na camada de saída

00:00:36.801 --> 00:00:39.200
vezes o erro
entre as unidades.

00:00:39.234 --> 00:00:40.234
Isso faz sentido.

00:00:40.267 --> 00:00:42.767
A unidade com conexão mais forte
ao nó de saída

00:00:42.801 --> 00:00:45.834
vai contribuir mais erro
para a saída final.

00:00:45.868 --> 00:00:48.367
Aqui estamos vendo o erro
vezes os pesos.

00:00:48.400 --> 00:00:51.367
É igual à propagação
das entradas pela da rede.

00:00:51.400 --> 00:00:54.067
As entradas vezes os pesos
entre as camadas.

00:00:54.100 --> 00:00:56.601
Em vez de propagar
as entradas para a frente,

00:00:56.634 --> 00:00:59.801
está propagando o erro
para trás pela rede.

00:00:59.834 --> 00:01:03.567
Pode ver este processo como
virar a rede ao contrário

00:01:03.601 --> 00:01:06.133
e usar o erro como entrada.

00:01:06.167 --> 00:01:08.634
Este método se chama
retropropagação.

00:01:08.667 --> 00:01:11.300
O processo é o mesmo
quando coloca mais camadas.

00:01:11.334 --> 00:01:14.133
Continua propagando o erro
pelas camadas.

00:01:14.167 --> 00:01:17.567
Retropropagação é fundamental
a como as redes neurais aprendem.

00:01:17.601 --> 00:01:19.400
É muito importante entender

00:01:19.434 --> 00:01:21.801
se vai construir modelos
de aprendizagem profunda.


<!-- udacity2.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Gradient Descent</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Implementing Gradient Descent</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Mean Squared Error Function.html">01. Mean Squared Error Function</a>
    </li>
    <li class="">
      <a href="02. Gradient Descent.html">02. Gradient Descent</a>
    </li>
    <li class="">
      <a href="03. Gradient Descent The Math.html">03. Gradient Descent: The Math</a>
    </li>
    <li class="">
      <a href="04. Gradient Descent The Code.html">04. Gradient Descent: The Code</a>
    </li>
    <li class="">
      <a href="05. Implementing Gradient Descent.html">05. Implementing Gradient Descent</a>
    </li>
    <li class="">
      <a href="06. Multilayer Perceptrons.html">06. Multilayer Perceptrons</a>
    </li>
    <li class="">
      <a href="07. Backpropagation.html">07. Backpropagation</a>
    </li>
    <li class="">
      <a href="08. Implementing Backpropagation.html">08. Implementing Backpropagation</a>
    </li>
    <li class="">
      <a href="09. Further Reading.html">09. Further Reading</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">02. Gradient Descent</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="gradient-descent-with-squared-errors">Gradient Descent with Squared Errors</h1>
<p>We want to find the weights for our neural networks. Let's start by thinking about the goal. The network needs to make predictions as close as possible to the real values. To measure this, we use a metric of how wrong the predictions are, the <strong>error</strong>. A common metric is the sum of the squared errors (SSE):</p>
<div class="mathquill">E = \frac{1}{2}\sum_{\mu} \sum_j \left[ y^{\mu}_j - \hat{y} ^{\mu}_j \right]^2</div>
<p>where <span class="mathquill ud-math">\hat y</span> is the prediction and <span class="mathquill ud-math">y</span> is the true value, and you take the sum over all output units <span class="mathquill ud-math">j</span> and another sum over all data points <span class="mathquill ud-math">\mu</span>. This might seem like a really complicated equation at first, but it's fairly simple once you understand the symbols and can say what's going on in words. </p>
<p>First, the inside sum over <span class="mathquill ud-math">j</span>. This variable <span class="mathquill ud-math">j</span> represents the output units of the network. So this inside sum is saying for each output unit, find the difference between the true value <span class="mathquill ud-math">y</span> and the predicted value from the network <span class="mathquill ud-math">\hat y</span>, then square the difference, then sum up all those squares.</p>
<p>Then the other sum over <span class="mathquill ud-math">\mu</span> is a sum over all the data points. So, for each data point you calculate the inner sum of the squared differences for each output unit. Then you sum up those squared differences for each data point. That gives you the overall error for all the output predictions for all the data points.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The SSE is a good choice for a few reasons. The square ensures the error is always positive and larger errors are penalized more than smaller errors. Also, it makes the math nice, always a plus.</p>
<p>Remember that the output of a neural network, the prediction, depends on the weights</p>
<div class="mathquill">\hat{y}^{\mu}_j = f \left( \sum_i{ w_{ij} x^{\mu}_i  }\right)</div>
<p>and accordingly the error depends on the weights</p>
<div class="mathquill">E = \frac{1}{2}\sum_{\mu} \sum_j \left[ y^{\mu}_j - f \left( \sum_i{ w_{ij} x^{\mu}_i  }\right) \right]^2</div>
<p>We want the network's prediction error to be as small as possible and the weights are the knobs we can use to make that happen. Our goal is to find weights <span class="mathquill ud-math">w_{ij}</span> that minimize the squared error <span class="mathquill ud-math">E</span>. To do this with a neural network, typically you'd use <strong>gradient descent</strong>.</p>
<h2 id="enter-gradient-descent">Enter Gradient Descent</h2>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3><p>Gradient Descent</p></h3>
  <video controls>
  <source src="02. Gradient Descent-29PmNG7fuuM.mp4" type="video/mp4">

  <track default="false" kind="subtitles" srclang="zh-CN" src="02. Gradient Descent-29PmNG7fuuM.zh-CN.vtt" label="zh-CN">
  <track default="true" kind="subtitles" srclang="en" src="02. Gradient Descent-29PmNG7fuuM.en.vtt" label="en">
  <track default="false" kind="subtitles" srclang="ja-JP" src="02. Gradient Descent-29PmNG7fuuM.ja-JP.vtt" label="ja-JP">
  <track default="false" kind="subtitles" srclang="pt-BR" src="02. Gradient Descent-29PmNG7fuuM.pt-BR.vtt" label="pt-BR">
</video>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>As Luis said, with gradient descent, we take multiple small steps towards our goal. In this case, we want to change the weights in steps that reduce the error. Continuing the analogy, the error is our mountain and we want to get to the bottom. Since the fastest way down a mountain is in the steepest direction, the steps taken should be in the direction that minimizes the error the most. We can find this direction by calculating the <em>gradient</em> of the squared error.</p>
<p><em>Gradient</em> is another term for rate of change or slope. If you need to brush up on this concept, check out Khan Academy's <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient" rel="noopener noreferrer" target="_blank">great lectures</a> on the topic.</p>
<p>To calculate a rate of change, we turn to calculus, specifically derivatives. A derivative of a function <span class="mathquill ud-math">f(x)</span> gives you another function <span class="mathquill ud-math">f'(x)</span> that returns the slope of <span class="mathquill ud-math">f(x)</span> at point <span class="mathquill ud-math">x</span>. For example, consider <span class="mathquill ud-math">f(x)=x^2</span>. The derivative of <span class="mathquill ud-math">x^2</span> is <span class="mathquill ud-math">f'(x) = 2x</span>. So, at <span class="mathquill ud-math">x = 2</span>, the slope is <span class="mathquill ud-math">f'(2) = 4</span>. Plotting this out, it looks like:</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/derivative-example.png" alt="Example of a gradient" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>Example of a gradient</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The gradient is just a derivative generalized to functions with more than one variable. We can use calculus to find the gradient at any point in our error function, which depends on the input weights. You'll see how the gradient descent step is derived on the next page.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Below I've plotted an example of the error of a neural network with two inputs, and accordingly, two weights. You can read this like a topographical map where points on a contour line have the same error and darker contour lines correspond to larger errors.</p>
<p>At each step, you calculate the error and the gradient, then use those to determine how much to change each weight. Repeating this process will eventually find weights that are close to the minimum of the error function, the black dot in the middle.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/gradient-descent.png" alt="Gradient descent steps to the lowest error" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>Gradient descent steps to the lowest error</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="caveats">Caveats</h1>
<p>Since the weights will just go wherever the gradient takes them, they can end up where the error is low, but not the lowest. These spots are called local minima. If the weights are initialized with the wrong values, gradient descent could lead the weights into a local minimum, illustrated below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/local-minima.png" alt="Gradient descent leading into a local minimum" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>Gradient descent leading into a local minimum</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>There are methods to avoid this, such as using <a href="https://distill.pub/2017/momentum/" rel="noopener noreferrer" target="_blank">momentum</a>.</p>
</div>

</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="03. Gradient Descent The Math.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://us-udacity.github.io/" target="_blank">【udacity2.0 】If you need more courses, please add wechat：udacity6</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('02. Gradient Descent')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>

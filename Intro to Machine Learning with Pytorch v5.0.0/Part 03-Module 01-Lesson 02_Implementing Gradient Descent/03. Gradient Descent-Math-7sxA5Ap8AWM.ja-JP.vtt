WEBVTT
Kind: captions
Language: ja-JP

00:00:00.750 --> 00:00:05.030
このような
単純なニューラルネットワークから出力を得る方法は

00:00:05.030 --> 00:00:05.910
習得済みです

00:00:05.910 --> 00:00:09.690
この出力を使って予測する必要がありますが
事前に正しい重みを知らずに予測するには

00:00:09.690 --> 00:00:14.030
ネットワークをどのように
構築すればよいでしょうか?

00:00:14.030 --> 00:00:17.530
これには真であることがわかっているデータを
ネットワークに提供し

00:00:17.530 --> 00:00:21.640
そのデータに一致するように
モデルパラメーターと重みを設定します

00:00:21.640 --> 00:00:25.330
まずは予測の正確度を示す
何らかの指標が必要です

00:00:25.330 --> 00:00:28.500
まずは
真の目標値yと

00:00:28.500 --> 00:00:32.220
ネットワーク出力ŷの誤差を使用します

00:00:32.220 --> 00:00:36.090
ただし 予測が高すぎると
誤差は負になり

00:00:36.090 --> 00:00:40.270
予測が同じだけ低すぎると
誤差は正になります

00:00:40.270 --> 00:00:42.370
これらの誤差が
どちらも正になるよう

00:00:43.420 --> 00:00:46.890
同じ方法で処理し
誤差を2乗します

00:00:46.890 --> 00:00:50.630
ここでなぜ絶対値を
とらないのでしょうか?

00:00:50.630 --> 00:00:53.250
平方値を使用する具体例の1つは
小さな誤差ではなく

00:00:53.250 --> 00:00:56.020
異常値にペナルティを課すことです

00:00:56.020 --> 00:00:59.690
また 誤差を2乗すると
後の計算がうまくいきます

00:00:59.690 --> 00:01:02.455
ただしこれは
1つの予測の誤差にすぎません

00:01:02.455 --> 00:01:06.860
データセット全体の誤差を
知る必要があります

00:01:06.860 --> 00:01:12.000
そのため
このようにμの合計で示される

00:01:13.010 --> 00:01:15.230
各データレコードの誤差を合計します

00:01:15.230 --> 00:01:20.190
これで ネットワークの
データセット全体の総合誤差がわかりました

00:01:20.190 --> 00:01:20.990
最後に

00:01:20.990 --> 00:01:25.100
1/2を乗算します
これにより計算がうまくいきます

00:01:25.100 --> 00:01:29.010
この公式は一般的に
二乗誤差の和と呼ばれます

00:01:29.010 --> 00:01:34.550
略して
SSEです

00:01:35.590 --> 00:01:38.910
ŷは
アクティベーション関数を通じて渡される

00:01:38.910 --> 00:01:41.780
重みと入力の
線形結合です

00:01:41.780 --> 00:01:45.240
ここに
それを代入すると

00:01:45.240 --> 00:01:48.500
誤差が重みwiと入力値xiに
依存することがわかります

00:01:49.580 --> 00:01:53.850
前述したように
データレコードはギリシャ文字μで表されます

00:01:53.850 --> 00:01:57.360
データは
2つのテーブル、配列、あるいは行列と

00:01:57.360 --> 00:01:59.180
考えることができます

00:01:59.180 --> 00:02:04.360
1つには入力データxが
もう1つには目標値yが含まれていると考えます

00:02:04.360 --> 00:02:07.940
各レコードが1行で表されているので
1行目はμ=1です

00:02:08.940 --> 00:02:11.160
合計誤差を計算するには

00:02:11.160 --> 00:02:15.590
これらの配列の行の値から
SSEを計算し

00:02:15.590 --> 00:02:17.440
その計算結果すべてを合計します

00:02:18.580 --> 00:02:21.800
SSEはネットワークの
パフォーマンスの指標です

00:02:21.800 --> 00:02:24.540
この値が大きい場合
ネットワークは正確度の低い予測をしています

00:02:24.540 --> 00:02:27.800
値が小さい場合
ネットワークは正確度の高い予測をしています

00:02:27.800 --> 00:02:30.700
つまりこの値を
できるだけ小さくする必要があります

00:02:30.700 --> 00:02:35.220
誤差を最小限に抑える方法を
わかりやすくするために

00:02:35.220 --> 00:02:38.620
データレコードが1つしかない単純な例を
考えてみましょう

00:02:38.620 --> 00:02:43.530
この単純なネットワークで
SSEを求めるには

00:02:43.530 --> 00:02:46.680
真の目標値yから予測ŷを減算し それを2乗して2で割ります

00:02:46.680 --> 00:02:51.440
予測を代入すると
誤差は重みの関数であることがわかります

00:02:51.440 --> 00:02:54.880
重みは ネットワークの予測を
変更するために使用できるドアノブです

00:02:54.880 --> 00:02:57.930
これは誤差全体に影響します

00:02:57.930 --> 00:03:00.919
目標は 誤差を最小限に抑える
重みを見つけることです

00:03:01.950 --> 00:03:05.580
これは重みが1つしかない
単純な誤差を表したものです

00:03:05.580 --> 00:03:08.790
目標はこのボウル型の底部分の
重さを見つけることです

00:03:08.790 --> 00:03:10.340
ランダムな重みから

00:03:10.340 --> 00:03:14.130
最小値に向けて
進んでいきます

00:03:14.130 --> 00:03:18.190
この方向は
勾配とは逆方向です

00:03:18.190 --> 00:03:21.580
勾配を1ステップずつ
下っていきます

00:03:21.580 --> 00:03:25.410
最終的に
重みから誤差関数の最小値を見つけます

00:03:25.410 --> 00:03:27.900
これが勾配降下法です

00:03:27.900 --> 00:03:32.490
重みを更新する必要があります
新しい重みwiは

00:03:32.490 --> 00:03:34.940
古い重みwiにこの重みステップであるΔwiを加算して求めます

00:03:36.140 --> 00:03:40.070
このギリシャ文字のΔは
一般的に変更を意味します

00:03:40.070 --> 00:03:43.169
重みステップは勾配
つまり各重みwiに対する

00:03:43.169 --> 00:03:47.543
誤差の微分係数に
比例します

00:03:47.543 --> 00:03:51.367
任意のスケーリングパラメーターを
追加します

00:03:51.367 --> 00:03:53.780
これにより 勾配降下法のステップのサイズを設定できます

00:03:53.780 --> 00:03:56.720
これは学習率と呼ばれ
ギリシャ文字ηで表されます

00:03:57.910 --> 00:04:01.410
この勾配を計算するには
多変数微積分が必要です

00:04:01.410 --> 00:04:04.970
偏微分係数をとるので
すでにご存じかもしれません

00:04:04.970 --> 00:04:08.890
ここでやっていることを
理解できなくても問題ありません

00:04:08.890 --> 00:04:12.540
もっと重要なのは
勾配降下法の概念と

00:04:12.540 --> 00:04:14.380
最終結果を理解することです

00:04:14.380 --> 00:04:18.620
復習が必要な場合は
Khan Academyの多変数微積分のレッスンをお勧めします

00:04:18.620 --> 00:04:21.130
そのリンクを載せておきます

00:04:21.130 --> 00:04:24.030
勾配を書き出すと
二乗誤差の重みに対する

00:04:24.030 --> 00:04:26.970
偏微分係数が
わかります

00:04:26.970 --> 00:04:30.330
ネットワーク出力ŷは
重みの関数です

00:04:30.330 --> 00:04:33.820
そのため
これは重みに依存する

00:04:33.820 --> 00:04:35.750
別の機能の関数です

00:04:35.750 --> 00:04:39.080
これには 連鎖法則を使って
微分係数を計算する必要があります

00:04:40.210 --> 00:04:43.120
連鎖法則を簡単に
振り返りましょう

00:04:43.120 --> 00:04:46.340
そのためzに対する関数pの
微分係数を求める必要があります

00:04:46.340 --> 00:04:51.150
pが zに依存する別の関数qに
依存する場合

00:04:51.150 --> 00:04:55.442
連鎖法則に従い
まずqに対するpの微分係数をとり

00:04:55.442 --> 00:04:58.825
次にその値に
zに対するqの微分係数を乗算します

00:04:58.825 --> 00:05:02.879
これを
通常の分数で表します

00:05:02.879 --> 00:05:07.535
分母と分子の∂qが相殺するため
∂p ∂zを得られます

00:05:07.535 --> 00:05:11.877
これは私たちの問題に関係しています
qを誤差y-ŷに pを

00:05:11.877 --> 00:05:14.811
二乗誤差に見立てられるからです

00:05:14.811 --> 00:05:18.842
その後 wiに対する微分係数をとり
qに対するpの微分係数をとると

00:05:18.842 --> 00:05:23.211
誤差そのものが
返されます

00:05:23.211 --> 00:05:27.100
指数の2が落ちて
1/2を相殺します

00:05:27.100 --> 00:05:31.140
これにより wiに対する
誤差の微分係数を得られます

00:05:32.290 --> 00:05:36.120
目標値yは重みには依存しませんが
ŷは依存します

00:05:37.130 --> 00:05:40.200
連鎖法則を
もう一度使用すると

00:05:40.200 --> 00:05:43.350
負符号が付き
ŷの偏微分係数が残されます

00:05:43.350 --> 00:05:47.650
ŷはhのアクティベーション関数に
等しいことを覚えていますか?

00:05:47.650 --> 00:05:51.810
hは重みと入力値の
線形結合です

00:05:51.810 --> 00:05:55.520
ŷの微分係数をとり
再び連鎖法則を使用します

00:05:55.520 --> 00:05:59.010
hのアクティベーション関数の
微分係数を

00:05:59.010 --> 00:06:02.390
線形結合の偏微分係数を
乗算します

00:06:02.390 --> 00:06:06.310
合計では 各重みに依存する項は
1つだけです

00:06:06.310 --> 00:06:07.090
これを重み1について書き出すと

00:06:07.090 --> 00:06:12.410
x1の最初の項だけが
重み1に依存することがわかります

00:06:12.410 --> 00:06:17.180
そのため 重み1に対する合計の
微分係数はx1です

00:06:17.180 --> 00:06:19.520
他のすべての項はゼロで

00:06:19.520 --> 00:06:24.860
wiに対するこの合計の
微分係数はxiです

00:06:24.860 --> 00:06:28.420
最後に
これらすべてをまとめると

00:06:28.420 --> 00:06:31.540
wiに対する二乗誤差の勾配は
誤差の負数×hのアクティベーション関数の

00:06:31.540 --> 00:06:37.410
微分係数×入力値xiで
求められます

00:06:37.410 --> 00:06:40.820
学習率η×誤差
×アクティベーション微分係数×入力値で

00:06:40.820 --> 00:06:44.590
重みステップが
求められます

00:06:44.590 --> 00:06:48.730
後で扱いやすいようにするために
誤差の項を定義します

00:06:48.730 --> 00:06:53.250
小文字のデルタδ=誤差×hのアクティベーション関数の
微分係数です

00:06:54.360 --> 00:06:58.660
さらに
重みの更新を書き出します

00:06:58.660 --> 00:07:03.320
wi=wi+学習率×誤差の項×入力iの値xiと
なります

00:07:04.350 --> 00:07:07.410
作業対象が
複数の出力ユニットである場合は

00:07:07.410 --> 00:07:10.820
単一の出力ネットワークから取得した
アーキテクチャーを積み重ねるが

00:07:10.820 --> 00:07:14.469
入力ユニットを新しい出力ユニットに結合すると
考えることができます

00:07:15.820 --> 00:07:19.940
これで 合計誤差に
各出力の誤差の合計が含まれます

00:07:20.770 --> 00:07:25.030
この勾配降下法のステップを拡張して
複数の出力を持つネットワークに適用することができます

00:07:25.030 --> 00:07:27.148
これには 下付き文字jで示される

00:07:27.148 --> 00:07:30.213
各アウトプットユニットの誤差項を
計算します

00:07:30.213 --> 00:07:33.000
次に これをコードに変換する方法を
紹介します

00:07:33.000 --> 00:07:35.267
これによりPythonとNumpyで勾配降下法を実装できます


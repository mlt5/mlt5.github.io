WEBVTT
Kind: captions
Language: en

00:00:04.400 --> 00:00:09.855
Welcome to this lesson on Principal Component Analysis or PCA.

00:00:09.855 --> 00:00:11.880
The techniques you've seen so far in

00:00:11.880 --> 00:00:15.085
this lesson had been about clustering your data together.

00:00:15.085 --> 00:00:20.469
This begins our journey on unsupervised learning techniques aimed at transforming data,

00:00:20.469 --> 00:00:22.279
rather than clustering it.

00:00:22.280 --> 00:00:25.410
The main use of transformation techniques like

00:00:25.410 --> 00:00:30.118
PCA is that it allows you to retain the informative parts of your data,

00:00:30.118 --> 00:00:31.589
but with less of it.

00:00:31.589 --> 00:00:33.280
Let's say you ask me,

00:00:33.280 --> 00:00:35.539
"Hey, what do you have for breakfast today?"

00:00:35.539 --> 00:00:37.570
I could say, "So,

00:00:37.570 --> 00:00:38.579
I was running late,

00:00:38.579 --> 00:00:40.314
I was going to grab a Nutrigain Bar,

00:00:40.314 --> 00:00:41.710
but then I noticed I was all out.

00:00:41.710 --> 00:00:43.825
So, I stopped by the coffee shop,

00:00:43.825 --> 00:00:45.795
and I grabbed a coffee,

00:00:45.795 --> 00:00:46.950
and then I was a little hungry,

00:00:46.950 --> 00:00:48.900
so I also grabbed a pastry."

00:00:48.899 --> 00:00:52.289
Alternatively, I could condense the answer to,

00:00:52.289 --> 00:00:54.344
I had a coffee and a pastry.

00:00:54.344 --> 00:00:59.354
The information needed to answer the question is available in both.

00:00:59.354 --> 00:01:03.774
But the second, pulls the necessary information from the first.

00:01:03.774 --> 00:01:06.995
This is how Principal Component Analysis works.

00:01:06.995 --> 00:01:11.939
PCA is the first of the techniques we will look at that uses what it knows about

00:01:11.939 --> 00:01:17.209
your data set to reduce it to only the parts that contain the most information.


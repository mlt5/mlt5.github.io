WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.490
Here, we'll look at an overview of how Gaussian Mixture Model Clustering works,

00:00:05.490 --> 00:00:09.041
and we'll use the dataset that we talked about in the previous video to do that.

00:00:09.041 --> 00:00:11.384
To do this clustering,

00:00:11.384 --> 00:00:15.074
we'll need to follow the Expectation-Maximization algorithm

00:00:15.074 --> 00:00:18.829
for Gaussian mixtures and that looks like this.

00:00:18.829 --> 00:00:23.070
The first step is that we initialize K Gaussian distributions.

00:00:23.070 --> 00:00:26.250
K, in this case, in this example is two because we know

00:00:26.250 --> 00:00:29.754
there were two original datasets, two original sources.

00:00:29.754 --> 00:00:36.009
The second step is to soft-cluster the data into the two Gaussians that we initialized.

00:00:36.009 --> 00:00:39.599
This is called the Expectation step or the E step.

00:00:39.600 --> 00:00:45.645
The third step is to re-estimate the Gaussians based on the soft clustering.

00:00:45.645 --> 00:00:48.865
This is called the Maximization step or M step.

00:00:48.865 --> 00:00:50.234
And then in the fourth step,

00:00:50.234 --> 00:00:54.509
we evaluate the log likelihood to check for convergence.

00:00:54.509 --> 00:00:57.644
If it converges, then that's all good and well.

00:00:57.645 --> 00:00:59.175
We return the results.

00:00:59.174 --> 00:01:03.899
If it does not, we go back to step two and we reiterate and then we go

00:01:03.899 --> 00:01:09.584
over and over again until we converge and we find the two Gaussians that we want.

00:01:09.584 --> 00:01:12.134
This is a quick overview of the algorithm.

00:01:12.135 --> 00:01:15.350
Let's look at it a little bit more closely in the next video.


WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.740
The first step in the expectation maximization algorithm for

00:00:04.740 --> 00:00:09.900
doing GMM clustering is to initialize the Gaussian distributions.

00:00:09.900 --> 00:00:12.870
In this example, we have two Gaussian distributions,

00:00:12.869 --> 00:00:15.239
so we need to give them initial values.

00:00:15.240 --> 00:00:21.675
Basically, we need to give them a mean and a standard deviation for each of the two.

00:00:21.675 --> 00:00:23.625
So there are a few ways to do that.

00:00:23.625 --> 00:00:28.949
The naive way to do it is to just set them to the average and mean of the dataset itself.

00:00:28.949 --> 00:00:30.960
So, that's one way to do it.

00:00:30.960 --> 00:00:35.984
In practice, however, a better way to do that is to run k-means on the dataset,

00:00:35.984 --> 00:00:39.435
and then we can use the clusters produced by

00:00:39.435 --> 00:00:43.094
k-means to initialize the Gaussian distributions.

00:00:43.094 --> 00:00:44.487
For this example here,

00:00:44.487 --> 00:00:46.859
however, let's pick random points,

00:00:46.859 --> 00:00:52.640
a random mean for each of the two Gaussians and a random variance.

00:00:52.640 --> 00:00:54.761
So we're going to be talking,

00:00:54.761 --> 00:00:55.950
instead of standard deviation,

00:00:55.950 --> 00:00:57.690
we'll be talking about variance.

00:00:57.689 --> 00:01:01.214
And variance is simply the standard deviation squared.

00:01:01.215 --> 00:01:03.465
And so these are the two Gaussians that we have,

00:01:03.465 --> 00:01:06.704
each with its own center and its own variance.

00:01:06.704 --> 00:01:09.629
We're still graphing the standard deviations here,

00:01:09.629 --> 00:01:15.074
but the calculations all talk about variance rather than the standard deviation.

00:01:15.075 --> 00:01:17.984
And with this, the first step is done,

00:01:17.984 --> 00:01:19.679
and now, we have two Gaussians.

00:01:19.680 --> 00:01:21.855
We can proceed to step two.

00:01:21.855 --> 00:01:25.995
Step two is to do a soft clustering of the data points.

00:01:25.995 --> 00:01:28.755
This is called the expectation step.

00:01:28.754 --> 00:01:30.780
Basically, this is our dataset.

00:01:30.780 --> 00:01:33.128
So we have N points.

00:01:33.128 --> 00:01:38.010
Each one has these two values for each feature.

00:01:38.010 --> 00:01:43.388
And so we have to calculate the membership of each point to each cluster.

00:01:43.388 --> 00:01:44.520
And this is not impossible.

00:01:44.519 --> 00:01:48.539
We can intuitively look at a point like this and say that it's more

00:01:48.540 --> 00:01:53.250
likely that it came from this Gaussian rather than this Gaussian,

00:01:53.250 --> 00:01:54.825
just because it's closer.

00:01:54.825 --> 00:01:58.890
But there is a way to actually calculate a number for that,

00:01:58.890 --> 00:02:01.769
and it looks a little bit like this.

00:02:01.769 --> 00:02:07.140
This is basically the probability density function of a normal distribution.

00:02:07.140 --> 00:02:10.740
This is where we can really benefit from what we know from statistics

00:02:10.740 --> 00:02:14.610
and probability about Gaussian and normal distributions.

00:02:14.610 --> 00:02:16.725
This is a term we can calculate,

00:02:16.724 --> 00:02:19.289
and this here is the same term as this,

00:02:19.289 --> 00:02:21.269
and this is slightly different.

00:02:21.270 --> 00:02:24.570
So these are given the first Gaussian,

00:02:24.569 --> 00:02:26.444
and these are given the second Gaussian.

00:02:26.444 --> 00:02:28.709
To clarify this, let's take an example.

00:02:28.710 --> 00:02:30.915
Let's look at the first point here.

00:02:30.914 --> 00:02:32.879
This is where it lies.

00:02:32.879 --> 00:02:36.000
So, we can first intuitively say that it's most

00:02:36.000 --> 00:02:39.900
likely that it came from Gaussian A rather than Gaussian B.

00:02:39.900 --> 00:02:43.080
So it belongs to Cluster A more than it does Cluster B.

00:02:43.080 --> 00:02:45.420
But let's go through the calculation and see

00:02:45.419 --> 00:02:50.250
what number we will assign the membership of it to.

00:02:50.250 --> 00:02:55.034
And so, the membership of point number one to

00:02:55.034 --> 00:03:01.109
Cluster A is denoted by E or the expectation of Z1a.

00:03:01.110 --> 00:03:04.425
So 1 is the number of number of point,

00:03:04.425 --> 00:03:06.540
A is the name of the cluster,

00:03:06.539 --> 00:03:11.864
Z is what is called a hidden variable or a latent variable.

00:03:11.865 --> 00:03:18.120
There are very clever mathematical trick that enable us to do all of this.

00:03:18.120 --> 00:03:22.890
We will not discuss Z and what Z does and how Z was used

00:03:22.889 --> 00:03:27.824
to derive this here because we want to look at an oversimplified example.

00:03:27.824 --> 00:03:31.500
So let's proceed to plug in X1,

00:03:31.500 --> 00:03:33.659
so this point here,

00:03:33.659 --> 00:03:40.979
into this probability density function given the parameters of Gaussian A or Cluster A.

00:03:40.979 --> 00:03:45.329
This is the probability density function of a normal distribution.

00:03:45.330 --> 00:03:47.610
So we just plug in X here,

00:03:47.610 --> 00:03:53.314
and we plug in the parameters of the Gaussian A here,

00:03:53.313 --> 00:03:55.704
and that would come up with a number,

00:03:55.705 --> 00:03:59.390
and that number is 0.001288.

00:03:59.389 --> 00:04:01.744
So we put that here and here,

00:04:01.745 --> 00:04:05.944
and we do the same by plugging in X, this point X,

00:04:05.944 --> 00:04:10.250
into the probability density function given the parameters of

00:04:10.250 --> 00:04:14.780
the second of Cluster B or the Gaussian B,

00:04:14.780 --> 00:04:17.375
and that gives us this value here.

00:04:17.375 --> 00:04:19.430
And so this is simple calculation.

00:04:19.430 --> 00:04:23.569
It turns out to be 0.99976.

00:04:23.569 --> 00:04:26.795
And what this means is that we are

00:04:26.795 --> 00:04:34.405
99.97 percent sure that this point belongs to this cluster here.

00:04:34.404 --> 00:04:38.664
And so that is the membership of point number 1 to Cluster A.

00:04:38.665 --> 00:04:42.460
From here, we can simply calculate its membership to cluster

00:04:42.459 --> 00:04:46.029
B since this value is between 0 and 1.

00:04:46.029 --> 00:04:50.919
So to calculate Cluster B membership is just 1 minus this value,

00:04:50.920 --> 00:04:54.685
or we can just plug it in to the same formula

00:04:54.685 --> 00:04:58.600
but to the different clusters and derive the same value here,

00:04:58.600 --> 00:05:02.295
especially if we have more than two clusters.

00:05:02.295 --> 00:05:06.689
And so this is the soft clustering of point number 1 with

00:05:06.689 --> 00:05:11.129
regards to Cluster A and Cluster B that we initialize in the first step.

00:05:11.129 --> 00:05:15.329
And so if we fill up the rest of these two columns,

00:05:15.329 --> 00:05:19.800
calculating the membership to each cluster for each point,

00:05:19.800 --> 00:05:22.829
we end up with something that looks like this.

00:05:22.829 --> 00:05:26.024
And let's take a closer look at this.

00:05:26.024 --> 00:05:32.497
So all of these red points have the highest membership with Cluster A.

00:05:32.497 --> 00:05:36.524
And all of these green points have the highest membership with Cluster B.

00:05:36.524 --> 00:05:38.609
And the points in the middle are probably

00:05:38.610 --> 00:05:43.410
50-50 or 40-60 membership between the two clusters.

00:05:43.410 --> 00:05:48.660
And with this, we have completed our soft clustering in step two,

00:05:48.660 --> 00:05:52.215
and so we're ready to go to step three.

00:05:52.214 --> 00:05:58.935
In step three, we will use this soft clustering to estimate new Gaussians.

00:05:58.935 --> 00:06:01.870
And that's what we'll look at in the next video.


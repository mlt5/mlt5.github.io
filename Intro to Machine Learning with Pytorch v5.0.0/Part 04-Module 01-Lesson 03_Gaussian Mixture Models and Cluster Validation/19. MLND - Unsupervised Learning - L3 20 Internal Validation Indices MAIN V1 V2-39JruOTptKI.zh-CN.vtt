WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.165
外部指标都很不错

00:00:03.165 --> 00:00:07.275
但它要求我们的数据集有标签

00:00:07.275 --> 00:00:09.025
但很多时候

00:00:09.025 --> 00:00:12.915
我们的数据集并没有标签

00:00:12.914 --> 00:00:16.289
更具体点 就是在非监督学习的情况下

00:00:16.289 --> 00:00:21.629
我们要借助于内部指标

00:00:21.629 --> 00:00:25.500
这些是内部指标的一些示例

00:00:25.500 --> 00:00:28.019
其中一些在 scikit-learn 中也有用到

00:00:28.019 --> 00:00:31.454
我们进一步来看轮廓系数

00:00:31.454 --> 00:00:36.299
它会给任何一个聚类在 -1 到 1 之间的值评分

00:00:36.299 --> 00:00:38.354
我们看个例子

00:00:38.354 --> 00:00:40.979
假如这是我们的数据集 我们对其进行聚类算法

00:00:40.979 --> 00:00:43.694
然后得到这个结果

00:00:43.695 --> 00:00:46.170
这个数据集是没有标签的

00:00:46.170 --> 00:00:51.885
所以我们不能与原始真相或原始标签进行比较

00:00:51.884 --> 00:00:55.777
我们必须要做并且能做的是

00:00:55.777 --> 00:01:01.109
借助内部指标并利用轮廓系数对其进行评分

00:01:01.109 --> 00:01:04.500
所以 计算轮廓系数的过程是这样

00:01:04.500 --> 00:01:07.140
每个点有一个轮廓系数

00:01:07.140 --> 00:01:08.670
对于数据集中的每个样本

00:01:08.670 --> 00:01:11.564
我们都可以计算轮廓系数

00:01:11.564 --> 00:01:14.594
计算公式为 b-a

00:01:14.594 --> 00:01:17.579
除以 a 和 b 中较大的一个

00:01:17.579 --> 00:01:20.715
所以 a 表示什么呢

00:01:20.715 --> 00:01:24.659
a 是同一个聚类中到其它样本的平均距离

00:01:24.659 --> 00:01:27.015
我们以这个点为例

00:01:27.015 --> 00:01:30.765
来计算这个点的轮廓系数

00:01:30.765 --> 00:01:35.099
由于 a 是同一个聚类中到其它样本的平均距离

00:01:35.099 --> 00:01:37.979
所以它等于这两个距离的平均值

00:01:37.980 --> 00:01:40.935
这就是 a 那么 b 又是什么呢

00:01:40.935 --> 00:01:47.445
b 是与它距离最近不同聚类中到样本的平均距离

00:01:47.444 --> 00:01:51.059
所以要得到 b 我们必须计算这个点

00:01:51.060 --> 00:01:55.290
到其它各个聚类中点的距离

00:01:55.290 --> 00:01:58.020
这个点到这个橘黄色的聚类之间的距离

00:01:58.019 --> 00:02:02.280
就是这个点到这两个橘黄色点距离的平均值

00:02:02.280 --> 00:02:05.969
这个点到绿色聚类的距离就是这个点到这两个绿色点之间距离的平均值

00:02:05.969 --> 00:02:10.169
我们进行比较可知

00:02:10.169 --> 00:02:12.554
这个绿色聚类位于这个点距离最近的聚类

00:02:12.555 --> 00:02:16.140
所以 b 等于这个点到这两个绿色点距离的平均值

00:02:16.139 --> 00:02:21.239
我们将 a、b 代入得到该点的轮廓系数

00:02:21.240 --> 00:02:24.600
我们计算聚类中每个点的轮廓系数

00:02:24.599 --> 00:02:29.370
然后取其平均值 就得到了整个聚类的轮廓系数

00:02:29.370 --> 00:02:33.719
我们来看一个轮廓系数的例子

00:02:33.719 --> 00:02:38.039
让它帮助我们找到聚类数据集的中聚类的最佳数量

00:02:38.039 --> 00:02:42.504
要算得 K 假如我们有这个

00:02:42.504 --> 00:02:45.504
原始的 没有标签的数据集

00:02:45.504 --> 00:02:51.549
如果我们进行 K 均值聚类且 K=2

00:02:51.550 --> 00:02:52.840
结果如下

00:02:52.840 --> 00:02:54.295
得到评分为

00:02:54.294 --> 00:02:57.280
0.798

00:02:57.280 --> 00:03:00.564
如果我们进行 K 均值聚类且 K=3

00:03:00.564 --> 00:03:01.870
结果如下

00:03:01.870 --> 00:03:05.064
得到的轮廓得分更高

00:03:05.064 --> 00:03:08.425
这更加符合我们的直觉

00:03:08.425 --> 00:03:10.225
如果 K=4 呢？

00:03:10.224 --> 00:03:15.340
我们希望 K=4 的轮廓得分比 K=3 的低

00:03:15.340 --> 00:03:18.439
结果确实如此

00:03:18.439 --> 00:03:22.204
其得分为 0.641

00:03:22.205 --> 00:03:23.915
所以在这三个 K 值中

00:03:23.914 --> 00:03:29.674
轮廓系数告诉我们 K=3 是最正确的选择

00:03:29.675 --> 00:03:35.660
这对我们来说是个好消息

00:03:35.659 --> 00:03:41.990
我们现在可以通过数学公式来验证对数据集中聚类数量的猜测了

00:03:41.990 --> 00:03:45.290
如果有多个我们无法想象并直观分裂的维度

00:03:45.289 --> 00:03:49.444
我们就可以这样做

00:03:49.444 --> 00:03:54.364
K=5 时的评分结果更糟糕 为 0.491

00:03:54.365 --> 00:03:57.509
当 K=4 或 K=5 时

00:03:57.508 --> 00:04:01.354
轮廓系数在进行罚分 因为在这样的聚类中

00:04:01.354 --> 00:04:04.429
这两个聚类之间的距离不够

00:04:04.430 --> 00:04:08.224
所以其结果为罚分 得分更低

00:04:08.224 --> 00:04:12.710
所以 在这个例子中 我们试着为 K 赋了四个值

00:04:12.710 --> 00:04:16.160
我们还可以为其赋更多的值 

00:04:16.160 --> 00:04:22.005
所以 这张图表显示的是 K 在 2 到 100 之间的得分

00:04:22.004 --> 00:04:29.889
我们可以看到 无论我们将数据集分割成多少个类

00:04:29.889 --> 00:04:33.264
K=3 时的轮廓得分最高

00:04:33.264 --> 00:04:36.759
我们再来看一个例子

00:04:36.759 --> 00:04:40.060
我们能很明显的发现 K=2 或 3

00:04:40.060 --> 00:04:43.030
是这个数据集最好的 K 值

00:04:43.029 --> 00:04:47.739
不管我们再如何增大聚类的数量

00:04:47.740 --> 00:04:53.530
轮廓得分都没有 K=2 或 3 时的好

00:04:53.529 --> 00:04:56.844
我们也可以利用轮廓系数来比较聚类算法

00:04:56.845 --> 00:05:00.345
比较它们对一个特定数据集的效果

00:05:00.345 --> 00:05:02.940
来看这个例子

00:05:02.939 --> 00:05:06.389
我们进行 K 均值聚类 得分为 0.801

00:05:06.389 --> 00:05:09.336
单连接算法的得分也一样

00:05:09.336 --> 00:05:11.729
全连接算法 Ward 最小方差法 DBSCAN

00:05:11.730 --> 00:05:15.435
其结果和得分都一样

00:05:15.435 --> 00:05:17.399
但我们来看看这个数据集

00:05:17.399 --> 00:05:22.199
进行 K 均值聚类 其得分为 0.637

00:05:22.199 --> 00:05:25.289
看起来还可以 但我们来看看单连接算法

00:05:25.290 --> 00:05:29.610
其得分接近于 0 

00:05:29.610 --> 00:05:34.580
一个聚类几乎占完了整个数据集 还不错

00:05:34.579 --> 00:05:36.639
我们可以放心了

00:05:36.639 --> 00:05:39.819
全连接算法也还不错

00:05:39.819 --> 00:05:42.519
Ward 最小方差法得分更好 DBSCAN...

00:05:42.519 --> 00:05:45.654
所以这个例子告诉我们 

00:05:45.654 --> 00:05:49.044
当我们使用 DBSCAN 时

00:05:49.045 --> 00:05:51.265
我们不应该使用轮廓系数

00:05:51.264 --> 00:05:56.860
更具体的说 它没有噪音的概念

00:05:56.860 --> 00:06:02.139
而 DBSCAN 并不总是倾向于

00:06:02.139 --> 00:06:04.870
这些紧凑的聚类

00:06:04.870 --> 00:06:09.879
这些聚类可以通过轮廓系数奖励聚类算法来获得结果

00:06:09.879 --> 00:06:12.310
我们可以发现与 GMM 类似的东西

00:06:12.310 --> 00:06:17.245
我认为这是一种很优美的分割方式

00:06:17.245 --> 00:06:19.240
尽管这是第四好的得分

00:06:19.240 --> 00:06:21.985
但是我非常喜欢这种分割

00:06:21.985 --> 00:06:26.410
最后一个例子显示了轮廓系数的一些缺点

00:06:26.410 --> 00:06:28.510
如果我们想

00:06:28.509 --> 00:06:33.939
使用两个环形的数据集 并与不同的聚类算法进行比较

00:06:33.939 --> 00:06:38.245
我们对其进行 K 均值聚类 得分为 0.35 

00:06:38.245 --> 00:06:40.795
但 Single Link 将其完美地分割 结果如下

00:06:40.795 --> 00:06:43.465
但其得分比 K 均值的低

00:06:43.464 --> 00:06:47.364
这是因为轮廓系数并不会奖励这样地分割数据集 

00:06:47.365 --> 00:06:51.879
或者这样的聚类

00:06:51.879 --> 00:06:55.540
它想找到从其它类分割开的

00:06:55.540 --> 00:06:59.740
那些紧凑 密集的环形类

00:06:59.740 --> 00:07:02.905
所以 如果你的数据外形或你想分割成的聚类外形是这样的

00:07:02.904 --> 00:07:06.849
使用轮廓系数的效果并不是很好

00:07:06.850 --> 00:07:12.355
尽管全连接的聚类很糟糕 但其得分稍好一点

00:07:12.355 --> 00:07:15.730
Ward 也是这样 DBSCAN 分割得很完美

00:07:15.730 --> 00:07:19.660
分割得非常漂亮 得分接近于 0

00:07:19.660 --> 00:07:21.860
这又是另一个证明我们处理 DBSCAN 时

00:07:21.860 --> 00:07:25.235
不应使用轮廓系数的例子

00:07:25.235 --> 00:07:28.475
GMM 和 K 均值的得分很相近

00:07:28.475 --> 00:07:34.310
所以处理 DBSCAN 时 不要使用轮廓系数

00:07:34.310 --> 00:07:36.290
但还有另一个算法

00:07:36.290 --> 00:07:38.840
在下面链接的论文里

00:07:38.839 --> 00:07:44.074
介绍了基于密度的聚类评价的内部指标

00:07:44.074 --> 00:07:47.014
其名为 DBCV 即 density-based clustering validation

00:07:47.014 --> 00:07:54.199
它在处理 DBSCAN 时的效果更好

